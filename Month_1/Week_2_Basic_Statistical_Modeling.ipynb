{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOZa/etrwY/EcS3bZENF1ZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical_Data_analysis/blob/main/Month_1/Week_2_Basic_Statistical_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analytics with Python: A 6-Month Journey\n",
        "By : [Abderrahim Benmoussa, Ph.D. ](https://https://github.com/wamaw123)\n",
        "\n",
        "Project's on Github : https://github.com/wamaw123/Biomedical_Data_analysis"
      ],
      "metadata": {
        "id": "7i8SP3ReLr0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Basic Statistical Modeling\n",
        "\n",
        "In this notebook, we'll dive deeper into basic statistical modeling to understand the associations between variables in biomedical datasets. With a primary focus on the 'diagnosis' variable, explore various statistical models suitable for binary outcomes.\n",
        "Like previous week we will proceed to do :\n",
        "1. **Data Importing**: We'll import the dataset from a GitHub repository.\n",
        "2. **Descriptive Statistics**: This will give us a reminder of the dataset's structure and characteristics.\n",
        "3. **Data Cleaning**: Since we will be working with the already clean dataset, we will just make sure it is indeed clean and has no issues.\n",
        "4. **Data exploration and visualization**: Exploring and visualizing the data will provide insights into its distribution and potential patterns.\n",
        "5. **Determination of the scientific question**: We will set our scientific question as a preliminary for setting up the hypothesis\n",
        "6. **Hypothesis postulation**: From the scientific question, we will posit a working hypothesis that we will strive to falsify using classical statistical models\n",
        "7. **Statistical test selection**: We select a statistical test based on the hypothesis we want to test and verify if the postulate for this test is respected.\n",
        "8. **Testing and analysis**: We will run the defined test and analyse the output"
      ],
      "metadata": {
        "id": "33r-b9O2cBwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first set up the environment by installing and importing the necessary libraries."
      ],
      "metadata": {
        "id": "6h6x9TshfchZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIbD25DnbYKc"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas numpy scipy statsmodels patsy dtale scikit-learn openai\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "## Data Manipulation\n",
        "import pandas as pd   # Essential for data manipulation and mathematical operations.\n",
        "import numpy as np    # Used for array-based operations and mathematical functions.\n",
        "\n",
        "## Visualization\n",
        "import matplotlib.pyplot as plt  # Fundamental plotting library.\n",
        "import seaborn as sns            # Builds on top of matplotlib for more advanced visualizations.\n",
        "\n",
        "## Statistical Testing\n",
        "from scipy import stats           # Library for scientific and technical computing.\n",
        "import statsmodels.api as sm      # Provides classes and functions for the estimation of many different statistical models.\n",
        "import statsmodels.formula.api as smf  # Formula-based API for the statsmodels library.\n",
        "from sklearn.model_selection import train_test_split  # Split data into training and test sets.\n",
        "\n",
        "## Interactive Exploration\n",
        "import dtale                      # Interactive tool for data frame exploration.\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "## LLM statistical test selection\n",
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load the week 2 dataset directly from GitHub and set it into a Pandas dataframe\n"
      ],
      "metadata": {
        "id": "tXnchmZHfu8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical_Data_analysis/26a597febf37711e75146e8781f4300b9651063a/Datasets/week_2/week_2.csv\"\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "pdQjhLwPf5ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Dataset\n",
        "\n",
        "This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. These features describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "The 3-dimensional space is described in the following reference: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets,\" Optimization Methods and Software, 1, 1992, 23-34].\n",
        "\n",
        "You can access this dataset from the following sources:\n",
        "\n",
        "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n",
        "- [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?resource=download)\n",
        "- UW CS FTP Server: `ftp.cs.wisc.edu`, Path: `cd math-prog/cpo-dataset/machine-learn/WDBC/`\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "We will use the same dataset as in previous week. The dataset contains the following attributes:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32) Ten real-valued features computed for each cell nucleus:\n",
        "\n",
        "   a) Radius (mean of distances from center to points on the perimeter)\n",
        "   b) Texture (standard deviation of gray-scale values)\n",
        "   c) Perimeter\n",
        "   d) Area\n",
        "   e) Smoothness (local variation in radius lengths)\n",
        "   f) Compactness (perimeter^2 / area - 1.0)\n",
        "   g) Concavity (severity of concave portions of the contour)\n",
        "   h) Concave points (number of concave portions of the contour)\n",
        "   i) Symmetry\n",
        "   j) Fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "### Class Distribution\n",
        "\n",
        "The class distribution in this dataset is as follows:\n",
        "- 357 benign\n",
        "- 212 malignant"
      ],
      "metadata": {
        "id": "UraLwWhrfXQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before any statistical modeling, a thorough exploratory data analysis (EDA) is crucial. This process will help us better understand the data's structure, identify any anomalies, and decide on subsequent steps. Since we did most of it last week, we will go for a quick version of it using basic exploration then D-Tale to verify everything is set properly.\n"
      ],
      "metadata": {
        "id": "S0fBEgk2gIOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic data overview\n",
        "print(data.head())\n",
        "print(data.describe())\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "3gjJigf5gmuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : The dataset contains diagnostic measurements for 569 breast cancer samples. Each sample has an ID and a diagnosis (such as malignant), along with 30 other features describing the characteristics of cell nuclei from biopsy images. These features range from mean values, standard errors, and \"worst\" values (largest mean value) of attributes like radius, texture, perimeter, and area, among others."
      ],
      "metadata": {
        "id": "KjiuuE63hxuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**\n",
        "\n",
        "Overall, the dataset seems well-populated without missing data, except for a column named 'Unnamed: 32', which is entirely empty and might be an artifact from data collection or processing. Before diving into further analysis, let's go ahead and drop it.\n"
      ],
      "metadata": {
        "id": "9VXza_-eiIDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Unnamed: 32' column\n",
        "if 'Unnamed: 32' in data.columns:\n",
        "    data = data.drop('Unnamed: 32', axis=1)\n",
        "    print(\"'Unnamed: 32' column has been successfully dropped.\")\n",
        "else:\n",
        "    print(\"'Unnamed: 32' column does not exist or has already been dropped.\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ZkKl9mEEibRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now observe and explore the entire dataset using D-Tale"
      ],
      "metadata": {
        "id": "i8rWOddRib7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a global variable to ensure D-Tale stays running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Show your data with D-Tale\n",
        "d = dtale.show(data)\n",
        "d"
      ],
      "metadata": {
        "id": "bD_yqfksgtLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : if D-tale does not work, one can try using ngrok but ngrok must be setup with tokken already available : see the details on how to do that [here](https://github.com/man-group/dtale#google-colab:~:text=If%20this%20does%20not%20work%20for%20you%20try%20using%20USE_NGROK%20which%20is%20described%20in%20the%20next%20section.)\n",
        "\n",
        "If D-Tale is still bugging, one can still go ahead and try some of the packages described in week 1, use directly the libraries available here or just go forward since it is not indispensible to visualize the data at this step as it is clean and we don't have set the question yet."
      ],
      "metadata": {
        "id": "DfYT1-Yfq9lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determination of the Scientific Question\n",
        "Given the biomedical nature of our dataset, a possible scientific question could be: \"Is there a significant association between the any of the features and the diagnosis variable?\"\n",
        "\n",
        "Let's select radius_mean as our independant variable for the sake of this exercise"
      ],
      "metadata": {
        "id": "_7kLHZIUusdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis Postulation\n",
        "Based on the above question, our hypothesis can be:\n",
        "\n",
        "Null Hypothesis (H0): There is no association between radius_mean and diagnosis.\n",
        "Alternative Hypothesis (H1): There is a significant association between radius_mean and diagnosis."
      ],
      "metadata": {
        "id": "GmZL4ZRzvpxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Test Selection\n",
        "Before diving into the analysis, we must choose the appropriate statistical test and verify if all assumptions for this test are respected. We can either use already available tools to select the proper test like [this one](https://inspect-lb.org/statistical-tests/) or play a little with large language models like bellow.\n",
        "\n",
        "You can get your chatGPT API [here](https://platform.openai.com/account/api-keys). Make sure to enter it between \"\"."
      ],
      "metadata": {
        "id": "g9HvPsZ6vs0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your API key from OpenAI (make sure not to share it or expose it publicly)\n",
        "# It's safer to use secrets or environment variables rather than hardcoding it. In colab, we can use this form field that one should delete postuse\n",
        "api_key = \"\" #@param {type:\"string\"} # Form field for API key input\n",
        "\n",
        "# Form fields for user to input details about their experiment\n",
        "experiment_objective = \"to know if there is a significant association between the continuous variable \\\"radius_mean\\\"  and the binary \\\"diagnosis\\\" variable\" #@param {type:\"string\"}\n",
        "num_of_groups =  1#@param {type:\"integer\"}\n",
        "type_of_data = \"Binary\" #@param [\"Continuous\", \"Categorical\", \"Ordinal\", \"Binary\"]\n",
        "is_data_paired = \"No\" #@param [\"Yes\", \"No\"]\n",
        "additional_details = \"Null Hypothesis (H0): There is no association between radius_mean and diagnosis. Alternative Hypothesis (H1):\" #@param {type:\"string\"}\n",
        "\n",
        "# Setting up the OpenAI API key from user input\n",
        "openai.api_key = api_key\n",
        "\n",
        "def AskGPT(prompt):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return print(completion.choices[0].message.content)\n",
        "\n",
        "# Constructing the prompt for ChatGPT\n",
        "prompt = (f\"I am conducting an experiment where the objective is: {experiment_objective}. \"\n",
        "         f\"I have {num_of_groups} group and the outcome / dependant data type is {type_of_data}. \"\n",
        "         f\"Is the data paired ? {is_data_paired}. Here are some additional details: {additional_details}. \"\n",
        "         \"Which statistical test or method would be best to use?\")\n",
        "\n",
        "# Interacting with GPT-3.5 Turbo using the provided function\n",
        "AskGPT(prompt)"
      ],
      "metadata": {
        "id": "ATT0J2rI1ke1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the statistical test selection tool listed above** : I found that the best test would be a pearson t-test comparing the two groups in the variable diagnosis for the variable radius-mean. If there is a statistically significant difference, then we can say there indeed an association. An alternative way to explore this question, would habe been to categorize the radius-mean variable but it is a lossy procedure than can lead to artificial segregation of some measures.\n",
        "\n",
        "\n",
        "**According to GPT**: In this case, the appropriate statistical test to determine the association between the continuous variable \"radius_mean\" and the binary \"diagnosis\" variable is an independent samples t-test. This test is used to compare the means of two independent groups. Since the data is not paired, an independent samples t-test is more appropriate than a paired samples t-test.\n",
        "\n",
        "The independent samples t-test will allow you to evaluate if there is a significant difference in the mean \"radius_mean\" between the two groups defined by the \"diagnosis\" variable (e.g., malignant and benign). It will provide a p-value that will help determine if you can reject or fail to reject the null hypothesis.\n",
        "\n",
        "However, it's worth noting that correlation analysis (e.g., Pearson correlation) can also provide information about the association between two variables, but in this case, a t-test is more suited to compare the means of two groups."
      ],
      "metadata": {
        "id": "eArlQ9bv7Yp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions of the t-test\n",
        "\n",
        "Since we chose a t-test, are some key assumptions for a t-test:\n",
        "\n",
        "- The dependent variable is continuous (interval/ratio data) : **yes**\n",
        "- The observations are independent of one another : **yes (two groups)**\n",
        "- The dependent variable is approximately normally distributed within each group. This assumes the data is not heavily skewed : **Let's verify this**\n",
        "- The variances of the dependent variable in each group are approximately equal (homogeneity of variance): **Lets assess by a Levene's test**\n",
        "- The samples are randomly selected and independent : **yes**\n",
        "- The sample sizes in each group are large enough. As a rule of thumb, each group should have at least 30 observations : **yes**\n",
        "\n",
        "\n",
        "Note : The pearson t-test can be robust to some violations of these assumptions, especially with large sample sizes. But it's important to check them to ensure the t-test is appropriate for the data and will give valid results. The type of t-test (one sample, independent samples, paired) has some additional specific assumptions too."
      ],
      "metadata": {
        "id": "xJbCM-c19MCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the assumptions and perform subsequent test we must first recode the diagnosis into a true binary variable (0,1)"
      ],
      "metadata": {
        "id": "3Xd5_8AV-L7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'diagnosis' to numeric (1 for 'M' and 0 for 'B')\n",
        "data['diagnosis_numeric'] = data['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Separate data into two groups based on 'diagnosis_numeric'\n",
        "group_M = data[data['diagnosis_numeric'] == 1]['radius_mean']\n",
        "group_B = data[data['diagnosis_numeric'] == 0]['radius_mean']"
      ],
      "metadata": {
        "id": "jPe0VPwe_K0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumption 1: Normality\n",
        "# We'll use the Shapiro-Wilk test for normality\n",
        "statistic_M, p_value_M = stats.shapiro(group_M)\n",
        "statistic_B, p_value_B = stats.shapiro(group_B)\n",
        "\n",
        "# Check if p-values are greater than the significance level (e.g., 0.05)\n",
        "normality_assumption_met_M = p_value_M > 0.05\n",
        "normality_assumption_met_B = p_value_B > 0.05\n",
        "\n",
        "# Assumption 2: Homogeneity of Variances\n",
        "# We'll use Levene's test for equality of variances\n",
        "statistic, p_value = stats.levene(group_M, group_B)\n",
        "\n",
        "# Check if p-value is greater than the significance level (e.g., 0.05)\n",
        "homogeneity_of_variances_met = p_value > 0.05\n",
        "\n",
        "# Display results\n",
        "\n",
        "print(\"Assumption 1 (Normality) for 'M' Met:\", p_value_M, normality_assumption_met_M)\n",
        "print(\"Assumption 1 (Normality) for 'B' Met:\", p_value_B, normality_assumption_met_B)\n",
        "print(\"Assumption 2 (Homogeneity of Variances) Met:\",p_value, homogeneity_of_variances_met)"
      ],
      "metadata": {
        "id": "0TiJLHog7fSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems the variable of interestin in the group M is not normally distributed. Let's look at the distributions."
      ],
      "metadata": {
        "id": "MLA5Y-Z1ABJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms for the 'M' (Malignant) and 'B' (Benign) groups\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(group_M, bins=20, alpha=0.5, label='Malignant', color='red')\n",
        "plt.hist(group_B, bins=20, alpha=0.5, label='Benign', color='blue')\n",
        "\n",
        "plt.title('Distribution of Radius Mean by Diagnosis')\n",
        "plt.xlabel('Radius Mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DFOTB6Fh_XEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distrubtion does not look perfectly normal indeed. With enough dataset, it could still be used for a t-test but for best practice we will look at workarounds.\n",
        "\n",
        "**Transform the Data:** we can try transforming the data to make it closer to a normal distribution. Common transformations include logarithmic, square root, or Box-Cox transformations. After transforming the data, we should be able to perform a t-test.\n",
        "\n",
        "**Use a Non-Parametric Test:** If the data cannot be transformed to meet the normality assumption, we can use a non-parametric test like the Mann-Whitney U test, which does not assume normality."
      ],
      "metadata": {
        "id": "K8WhF--_AJKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-test with transformation\n"
      ],
      "metadata": {
        "id": "Bhj443J2Am6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformation = \"Logarithm\" # @param [\"None\", \"Logarithm\", \"Square Root\", \"Box-Cox\"]\n",
        "\n",
        "# Function to perform t-test with or without transformation\n",
        "def perform_t_test(transformation):\n",
        "    if transformation == 'None':\n",
        "        group_M_transformed = group_M\n",
        "        group_B_transformed = group_B\n",
        "    elif transformation == 'Logarithm':\n",
        "        group_M_transformed = np.log(group_M)\n",
        "        group_B_transformed = np.log(group_B)\n",
        "    elif transformation == 'Square Root':\n",
        "        group_M_transformed = np.sqrt(group_M)\n",
        "        group_B_transformed = np.sqrt(group_B)\n",
        "    elif transformation == 'Box-Cox':\n",
        "        group_M_transformed, _ = stats.boxcox(group_M)\n",
        "        group_B_transformed, _ = stats.boxcox(group_B)\n",
        "\n",
        "    # Perform t-test\n",
        "    t_statistic, p_value = stats.ttest_ind(group_M_transformed, group_B_transformed)\n",
        "\n",
        "    # Display the p-value\n",
        "    print(f\"T-Test p-value ({transformation}): {p_value}\")\n",
        "\n",
        "    # Plot histograms of the transformed data\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(group_M_transformed, bins=20, alpha=0.5, color='blue', label='Malignant')\n",
        "    plt.hist(group_B_transformed, bins=20, alpha=0.5, color='green', label='Benign')\n",
        "    plt.xlabel('Transformed Values')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Distribution of Transformed Data ({transformation})')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "# Initial test with the selected transformation\n",
        "perform_t_test(transformation)\n"
      ],
      "metadata": {
        "id": "My69f02yBq2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is not much improvement on the P-value depending on the tansformation. This is very likely because we have so many data points that the normality assumptions is not a limiting one at this scale.\n",
        "\n",
        "In all cases, there seems to be a significant difference between the two groups in terms of mean radius which confirms that there is a strong association between this feature and the outcome."
      ],
      "metadata": {
        "id": "7iwlbZldDmXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mann-Whitney U test\n",
        "\n",
        "As an alternative, we can run a Mann-Whitney test, that is not influenced by normality since it is non-parametric."
      ],
      "metadata": {
        "id": "TlBWlEnBB_q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Mann-Whitney U test\n",
        "statistic, p_value = stats.mannwhitneyu(group_M, group_B)\n",
        "\n",
        "# Display the results\n",
        "print(\"Mann-Whitney U Test Statistic:\", statistic)\n",
        "print(\"Mann-Whitney U Test p-value:\", p_value)"
      ],
      "metadata": {
        "id": "8sNmz1dzCMd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This indicates a highly significant difference between the two groups, suggesting that the 'radius_mean' is not equal between the 'M' (Malignant) and 'B' (Benign) diagnosis groups. So, as for the previous t-test, there seems to be a significant difference between the two groups in terms of mean radius which confirms that there is a strong association between this feature and the outcome."
      ],
      "metadata": {
        "id": "DkXfx6HTEx-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions and perspectives"
      ],
      "metadata": {
        "id": "Huz2zl7BE7x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explored Basic Statistical Modeling to answer a simple statistical test of hypothesis.\n",
        "\n",
        "We investigated whether there is a significant association between the characteristic \"radius_mean\" and the variable \"diagnosis\" in a biomedical dataset.\n",
        "After performing a Mann-Whitney U test, our conclusions are convincing. The Mann-Whitney U test statistic gave a value of 70955.0 and the associated p-value was incredibly low, around 2.69e-68. This tiny p-value indicates a highly significant difference between the groups, which strongly supports rejection of the null hypothesis.\n",
        "\n",
        "Therefore, based on this analysis, we have sufficient evidence to conclude that \"radius_mean\" is significantly associated with the \"diagnosis\" variable in this biomedical dataset. This result is consistent with our alternative hypothesis, suggesting that \"radius_mean\" may be a valuable feature for discriminating benign and malignant diagnoses in this context. Further analysis and exploration of this association could provide valuable information for biomedical research and diagnostic applications.\n",
        "\n",
        "Few notes :    \n",
        "- While I explored the dataset, I selected a variable among others at random, without specifically looking into its potential association by classic correlations for instance. This is because i wanted to set this exploration in a scientific context where there was a specific initial scientific question set without a priori decisions which is the case in many scientific studies. In these cases, it is not proper to explore the dataset for finding the best associated feature and then set the question. One must proceed from the question that helped design the scientific experiment and then test the question through hypothesis testing.\n",
        "- In that, the scientific question is usually set before any data collection and guides it and also before any data-wrangling, usually, the statistical test is already selected a priori\n",
        "- There are many scientific tests and questions. Here I choose to illustrate the simple groupe comparison but anything can go from here using the same approach.\n",
        "- Subject matter experts are indispensible in both defining the scientific question and moving forward with the testing. Since I have expertise in both data science and the specific cancerology subject, I gained a lot of time in setting up this exercice.\n",
        "- One thing to improve would be"
      ],
      "metadata": {
        "id": "5PdTFmI5FJb8"
      }
    }
  ]
}