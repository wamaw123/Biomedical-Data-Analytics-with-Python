{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical_Data_analysis/blob/main/Month_1/Week_2_Basic_Statistical_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Basic Statistical Modeling\n",
        "\n",
        "In this notebook, we'll dive deeper into basic statistical modeling to understand the associations between variables in biomedical datasets. With a primary focus on the 'diagnosis' variable, explore various statistical models suitable for binary outcomes.\n",
        "Like previous week we will proceed to do :\n",
        "1. **Data Importing**: We'll import the dataset from a GitHub repository.\n",
        "2. **Descriptive Statistics**: This will give us a reminder of the dataset's structure and characteristics.\n",
        "3. **Data Cleaning**: Since we will be working with the already clean dataset, we will just make sure it is indeed clean and has no issues.\n",
        "4. **Data exploration and visualization**: Exploring and visualizing the data will provide insights into its distribution and potential patterns.\n",
        "5. **Determination of the scientific question**: We will set our scientific question as a preliminary for setting up the hypothesis\n",
        "6. **Hypothesis postulation**: From the scientific question, we will posit a working hypothesis that we will strive to falsify using classical statistical models\n",
        "7. **Statistical test selection**: We select a statistical test based on the hypothesis we want to test and verify if the postulate for this test is respected.\n",
        "8. **Testing and analysis**: We will run the defined test and analyse the output"
      ],
      "metadata": {
        "id": "33r-b9O2cBwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first set up the environment by installing and importing the necessary libraries."
      ],
      "metadata": {
        "id": "6h6x9TshfchZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIbD25DnbYKc"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas numpy scipy statsmodels patsy dtale\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import dtale                      # Interactive tool for data frame exploration.\n",
        "import dtale.app as dtale_app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load the week 2 dataset directly from GitHub and set it into a Pandas dataframe\n"
      ],
      "metadata": {
        "id": "tXnchmZHfu8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical_Data_analysis/26a597febf37711e75146e8781f4300b9651063a/Datasets/week_2/week_2.csv\"\n",
        "data = pd.read_csv(url)\n"
      ],
      "metadata": {
        "id": "pdQjhLwPf5ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Dataset\n",
        "\n",
        "This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. These features describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "The 3-dimensional space is described in the following reference: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets,\" Optimization Methods and Software, 1, 1992, 23-34].\n",
        "\n",
        "You can access this dataset from the following sources:\n",
        "\n",
        "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n",
        "- [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?resource=download)\n",
        "- UW CS FTP Server: `ftp.cs.wisc.edu`, Path: `cd math-prog/cpo-dataset/machine-learn/WDBC/`\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "We will use the same dataset as in previous week. The dataset contains the following attributes:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32) Ten real-valued features computed for each cell nucleus:\n",
        "\n",
        "   a) Radius (mean of distances from center to points on the perimeter)\n",
        "   b) Texture (standard deviation of gray-scale values)\n",
        "   c) Perimeter\n",
        "   d) Area\n",
        "   e) Smoothness (local variation in radius lengths)\n",
        "   f) Compactness (perimeter^2 / area - 1.0)\n",
        "   g) Concavity (severity of concave portions of the contour)\n",
        "   h) Concave points (number of concave portions of the contour)\n",
        "   i) Symmetry\n",
        "   j) Fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "### Class Distribution\n",
        "\n",
        "The class distribution in this dataset is as follows:\n",
        "- 357 benign\n",
        "- 212 malignant"
      ],
      "metadata": {
        "id": "UraLwWhrfXQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before any statistical modeling, a thorough exploratory data analysis (EDA) is crucial. This process will help us better understand the data's structure, identify any anomalies, and decide on subsequent steps. Since we did most of it last week, we will go for a quick version of it using basic exploration then D-Tale to verify everything is set properly.\n"
      ],
      "metadata": {
        "id": "S0fBEgk2gIOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic data overview\n",
        "print(data.head())\n",
        "print(data.describe())\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "3gjJigf5gmuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : The dataset contains diagnostic measurements for 569 breast cancer samples. Each sample has an ID and a diagnosis (such as malignant), along with 30 other features describing the characteristics of cell nuclei from biopsy images. These features range from mean values, standard errors, and \"worst\" values (largest mean value) of attributes like radius, texture, perimeter, and area, among others."
      ],
      "metadata": {
        "id": "KjiuuE63hxuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Cleaning**\n",
        "\n",
        "Overall, the dataset seems well-populated without missing data, except for a column named 'Unnamed: 32', which is entirely empty and might be an artifact from data collection or processing. Before diving into further analysis, let's go ahead and drop it.\n"
      ],
      "metadata": {
        "id": "9VXza_-eiIDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Unnamed: 32' column\n",
        "if 'Unnamed: 32' in data.columns:\n",
        "    data = data.drop('Unnamed: 32', axis=1)\n",
        "    print(\"'Unnamed: 32' column has been successfully dropped.\")\n",
        "else:\n",
        "    print(\"'Unnamed: 32' column does not exist or has already been dropped.\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ZkKl9mEEibRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now observe and explore the entire dataset using D-Tale"
      ],
      "metadata": {
        "id": "i8rWOddRib7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtale\n",
        "import dtale\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "# Set a global variable to ensure D-Tale stays running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Show your data with D-Tale\n",
        "d = dtale.show(data)\n",
        "d"
      ],
      "metadata": {
        "id": "bD_yqfksgtLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XW1fPlJvhw6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will perform a orrelation analysis which provides a preliminary insight into the potential associations between variables. Let's visualize these correlations.\n"
      ],
      "metadata": {
        "id": "_RXshqV1gZ-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data, hue='diagnosis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_IELYeWumCF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing and visualizing the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yhn7gjVCge4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}