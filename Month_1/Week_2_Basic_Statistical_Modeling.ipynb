{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPQRnpEBOVZ9WJfoLBeT8lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical_Data_analysis/blob/main/Month_1/Week_2_Basic_Statistical_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Basic Statistical Modeling\n",
        "\n",
        "In this notebook, we'll dive deeper into basic statistical modeling to understand the associations between variables in biomedical datasets. With a primary focus on the 'diagnosis' variable, explore various statistical models suitable for binary outcomes.\n",
        "Like previous week we will proceed to do :\n",
        "1. **Data Importing**: We'll import the dataset from a GitHub repository.\n",
        "2. **Descriptive Statistics**: This will give us a reminder of the dataset's structure and characteristics.\n",
        "3. **Data Cleaning**: Since we will be working with the already clean dataset, we will just make sure it is indeed clean and has no issues.\n",
        "4. **Data exploration and visualization**: Exploring and visualizing the data will provide insights into its distribution and potential patterns.\n",
        "5. **Determination of the scientific question**: We will set our scientific question as a preliminary for setting up the hypothesis\n",
        "6. **Hypothesis postulation**: From the scientific question, we will posit a working hypothesis that we will strive to falsify using classical statistical models\n",
        "7. **Statistical test selection**: We select a statistical test based on the hypothesis we want to test and verify if the postulate for this test is respected.\n",
        "8. **Testing and analysis**: We will run the defined test and analyse the output"
      ],
      "metadata": {
        "id": "33r-b9O2cBwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first set up the environment by installing and importing the necessary libraries."
      ],
      "metadata": {
        "id": "6h6x9TshfchZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIbD25DnbYKc"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas numpy scipy statsmodels patsy dtale\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import dtale                      # Interactive tool for data frame exploration.\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "## Visualization\n",
        "import matplotlib.pyplot as plt  # Fundamental plotting library.\n",
        "import seaborn as sns            # Builds on top of matplotlib for more advanced visualizations.\n",
        "\n",
        "# Installing libraries\n",
        "!pip install statsmodels scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load the week 2 dataset directly from GitHub and set it into a Pandas dataframe\n"
      ],
      "metadata": {
        "id": "tXnchmZHfu8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical_Data_analysis/26a597febf37711e75146e8781f4300b9651063a/Datasets/week_2/week_2.csv\"\n",
        "data = pd.read_csv(url)\n"
      ],
      "metadata": {
        "id": "pdQjhLwPf5ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Dataset\n",
        "\n",
        "This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. These features describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "The 3-dimensional space is described in the following reference: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets,\" Optimization Methods and Software, 1, 1992, 23-34].\n",
        "\n",
        "You can access this dataset from the following sources:\n",
        "\n",
        "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n",
        "- [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?resource=download)\n",
        "- UW CS FTP Server: `ftp.cs.wisc.edu`, Path: `cd math-prog/cpo-dataset/machine-learn/WDBC/`\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "We will use the same dataset as in previous week. The dataset contains the following attributes:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32) Ten real-valued features computed for each cell nucleus:\n",
        "\n",
        "   a) Radius (mean of distances from center to points on the perimeter)\n",
        "   b) Texture (standard deviation of gray-scale values)\n",
        "   c) Perimeter\n",
        "   d) Area\n",
        "   e) Smoothness (local variation in radius lengths)\n",
        "   f) Compactness (perimeter^2 / area - 1.0)\n",
        "   g) Concavity (severity of concave portions of the contour)\n",
        "   h) Concave points (number of concave portions of the contour)\n",
        "   i) Symmetry\n",
        "   j) Fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "### Class Distribution\n",
        "\n",
        "The class distribution in this dataset is as follows:\n",
        "- 357 benign\n",
        "- 212 malignant"
      ],
      "metadata": {
        "id": "UraLwWhrfXQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before any statistical modeling, a thorough exploratory data analysis (EDA) is crucial. This process will help us better understand the data's structure, identify any anomalies, and decide on subsequent steps. Since we did most of it last week, we will go for a quick version of it using basic exploration then D-Tale to verify everything is set properly.\n"
      ],
      "metadata": {
        "id": "S0fBEgk2gIOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic data overview\n",
        "print(data.head())\n",
        "print(data.describe())\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "3gjJigf5gmuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : The dataset contains diagnostic measurements for 569 breast cancer samples. Each sample has an ID and a diagnosis (such as malignant), along with 30 other features describing the characteristics of cell nuclei from biopsy images. These features range from mean values, standard errors, and \"worst\" values (largest mean value) of attributes like radius, texture, perimeter, and area, among others."
      ],
      "metadata": {
        "id": "KjiuuE63hxuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Cleaning**\n",
        "\n",
        "Overall, the dataset seems well-populated without missing data, except for a column named 'Unnamed: 32', which is entirely empty and might be an artifact from data collection or processing. Before diving into further analysis, let's go ahead and drop it.\n"
      ],
      "metadata": {
        "id": "9VXza_-eiIDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Unnamed: 32' column\n",
        "if 'Unnamed: 32' in data.columns:\n",
        "    data = data.drop('Unnamed: 32', axis=1)\n",
        "    print(\"'Unnamed: 32' column has been successfully dropped.\")\n",
        "else:\n",
        "    print(\"'Unnamed: 32' column does not exist or has already been dropped.\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ZkKl9mEEibRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now observe and explore the entire dataset using D-Tale"
      ],
      "metadata": {
        "id": "i8rWOddRib7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtale\n",
        "import dtale\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "# Set a global variable to ensure D-Tale stays running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Show your data with D-Tale\n",
        "d = dtale.show(data)\n",
        "d"
      ],
      "metadata": {
        "id": "bD_yqfksgtLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : if D-tale does not work, one can try using ngrok but ngrok must be setup with tokken already available : see the details on how to do that [here](https://github.com/man-group/dtale#google-colab:~:text=If%20this%20does%20not%20work%20for%20you%20try%20using%20USE_NGROK%20which%20is%20described%20in%20the%20next%20section.)\n",
        "\n",
        "If D-Tale is still bugging, one can still go ahead and try some of the packages described in week 1, use directly the libraries available here or just go forward since it is not indispensible to visualize the data at this step as it is clean and we don't have set the question yet."
      ],
      "metadata": {
        "id": "DfYT1-Yfq9lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determination of the Scientific Question\n",
        "Given the biomedical nature of our dataset, a possible scientific question could be: \"Is there a significant association between the any of the features and the diagnosis variable?\"\n",
        "\n",
        "Let's select radius_mean as our independant variable for the sake of this exercise"
      ],
      "metadata": {
        "id": "_7kLHZIUusdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis Postulation\n",
        "Based on the above question, our hypothesis can be:\n",
        "\n",
        "Null Hypothesis (H0): There is no association between radius_mean and diagnosis.\n",
        "Alternative Hypothesis (H1): There is a significant association between radius_mean and diagnosis."
      ],
      "metadata": {
        "id": "GmZL4ZRzvpxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Test Selection\n",
        "Before diving into the analysis, we must choose the appropriate statistical test and verify if all assumptions for this test are respected. We can either use already available tools to select the proper test like [this one](https://https://inspect-lb.org/statistical-tests/) or play a little with large language models like bellow.\n",
        "\n",
        "You can get your chatGPT API [here](https://platform.openai.com/account/api-keys). Make sure to enter it between \"\"."
      ],
      "metadata": {
        "id": "g9HvPsZ6vs0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "# Set your API key from OpenAI (make sure not to share it or expose it publicly)\n",
        "# It's safer to use secrets or environment variables rather than hardcoding it.\n",
        "api_key = \"\" #@param {type:\"string\"} # Form field for API key input\n",
        "\n",
        "# Form fields for user to input details about their experiment\n",
        "experiment_objective = \"to know if there is a significant association between the continuous variable \\\"radius_mean\\\"  and the binary \\\"diagnosis\\\" variable\" #@param {type:\"string\"}\n",
        "num_of_groups =  1#@param {type:\"integer\"}\n",
        "type_of_data = \"Binary\" #@param [\"Continuous\", \"Categorical\", \"Ordinal\", \"Binary\"]\n",
        "is_data_paired = \"No\" #@param [\"Yes\", \"No\"]\n",
        "additional_details = \"Null Hypothesis (H0): There is no association between radius_mean and diagnosis. Alternative Hypothesis (H1):\" #@param {type:\"string\"}\n",
        "\n",
        "# Setting up the OpenAI API key from user input\n",
        "openai.api_key = api_key\n",
        "\n",
        "def chatWithGPT(prompt):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return print(completion.choices[0].message.content)\n",
        "\n",
        "# Constructing the prompt for ChatGPT\n",
        "prompt = (f\"I am conducting an experiment where the objective is: {experiment_objective}. \"\n",
        "         f\"I have {num_of_groups} group and the outcome / dependant data type is {type_of_data}. \"\n",
        "         f\"Is the data paired ? {is_data_paired}. Here are some additional details: {additional_details}. \"\n",
        "         \"Which statistical test or method would be best to use?\")\n",
        "\n",
        "# Interacting with GPT-3.5 Turbo using the provided function\n",
        "chatWithGPT(prompt)"
      ],
      "metadata": {
        "id": "ATT0J2rI1ke1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the statistical test selection tool listed above** : I found that the best test would be a pearson t-test comparing the two groups in the variable diagnosis for the variable radius-mean. If there is a statistically significant difference, then we can say there indeed an association.\n",
        "\n",
        "\n",
        "**According to GPT**: In this case, the appropriate statistical test to determine the association between the continuous variable \"radius_mean\" and the binary \"diagnosis\" variable is an independent samples t-test. This test is used to compare the means of two independent groups. Since the data is not paired, an independent samples t-test is more appropriate than a paired samples t-test.\n",
        "\n",
        "The independent samples t-test will allow you to evaluate if there is a significant difference in the mean \"radius_mean\" between the two groups defined by the \"diagnosis\" variable (e.g., malignant and benign). It will provide a p-value that will help determine if you can reject or fail to reject the null hypothesis.\n",
        "\n",
        "However, it's worth noting that correlation analysis (e.g., Pearson correlation) can also provide information about the association between two variables, but in this case, a t-test is more suited to compare the means of two groups."
      ],
      "metadata": {
        "id": "eArlQ9bv7Yp7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TiJLHog7fSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}