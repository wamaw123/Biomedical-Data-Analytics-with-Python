{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNcQkaCEYVuhXIfi/1zfvLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical_Data_analysis/blob/main/Month_1/Week_1_Data_Importing_and_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1: Data Importing and Cleaning\n",
        "\n",
        "In this notebook, we'll focus on the foundational steps of any data analysis process:\n",
        "1. **Data Importing**: We'll import a biomedical dataset from a GitHub repository.\n",
        "2. **Descriptive Statistics**: This will give us a preliminary understanding of the dataset's structure and characteristics.\n",
        "3. **Data Cleaning**: We'll handle missing values and outliers to ensure the data's quality.\n",
        "4. **Data Visualization**: Visualizing the data will provide insights into its distribution and potential patterns.\n",
        "5. **Normalization and Standardization**: We'll transform the data to prepare it for future analysis.\n",
        "\n",
        "Let's begin by importing the necessary libraries and the dataset.\n"
      ],
      "metadata": {
        "id": "FxAjKrnLXnta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas_profiling\n",
        "!pip install dtale\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import pandas_profiling\n",
        "from google.colab import files\n",
        "\n",
        "# Set up the notebook for visualizations\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical_Data_analysis/c072fdafc2b2abe4e002f8611f80bcf5fd8366b8/Datasets/Week_1/week_1.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "bLt69CiJXndE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive Statistics and Data Exploration\n",
        "\n",
        "In this section, we'll delve deep into our dataset to understand its structure, characteristics, and potential issues. This includes understanding basic information, central tendencies, visualizations, and more.\n"
      ],
      "metadata": {
        "id": "TYtpPjFcXyQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Information\n",
        "print(\"Dataset Shape:\", data.shape)\n",
        "print(\"\\nData Types:\\n\", data.dtypes)\n",
        "print(\"\\nUnique Values in Each Column:\\n\", data.nunique())\n",
        "\n",
        "# Central Tendency and Dispersion\n",
        "print(\"\\nMean:\\n\", data.mean(numeric_only=True))\n",
        "print(\"\\nMedian:\\n\", data.median(numeric_only=True))\n",
        "print(\"\\nMode:\\n\", data.mode(numeric_only=True).iloc[0])\n",
        "print(\"\\nStandard Deviation:\\n\", data.std(numeric_only=True))\n",
        "print(\"\\nVariance:\\n\", data.var(numeric_only=True))\n",
        "\n",
        "# For the range, we'll only consider numeric columns to avoid the TypeError\n",
        "numeric_data = data.select_dtypes(include=[np.number])\n",
        "print(\"\\nRange:\\n\", numeric_data.max() - numeric_data.min())\n",
        "print(\"\\nQuartiles:\\n\", data.quantile([0.25, 0.5, 0.75], numeric_only=True))\n",
        "\n",
        "# Missing Values\n",
        "print(\"\\nMissing Values Count:\\n\", data.isnull().sum())\n",
        "print(\"\\nPercentage of Missing Values:\\n\", (data.isnull().sum() / len(data)) * 100)\n",
        "\n",
        "# Visualizations\n",
        "numeric_data.hist(figsize=(12, 10))\n",
        "plt.suptitle(\"Histograms of Data Columns\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.boxplot(data=numeric_data)\n",
        "plt.title(\"Boxplots of Data Columns\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Additional Insights\n",
        "print(\"\\nSkewness:\\n\", numeric_data.skew())\n",
        "print(\"\\nKurtosis:\\n\", numeric_data.kurtosis())"
      ],
      "metadata": {
        "id": "7AQpA57TX1wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Data Exploration with pandas_profiling\n",
        "\n",
        "This does the trick but it is not very friendly. For a more interactive and comprehensive overview of our dataset, we'll use the `pandas_profiling` package. This tool generates an interactive HTML report that provides a deep dive into each column, correlations, missing values, and much more.\n",
        "\n",
        "## NOTE : Panda profiling is a powerfull tool but the file can be very large and exploring it can lead to buggy behavior.\n"
      ],
      "metadata": {
        "id": "v-URfgw0cpwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the profile report\n",
        "profile = pandas_profiling.ProfileReport(numeric_data)\n",
        "profile_file_path = \"data_profile_report.html\"\n",
        "profile.to_file(output_file=profile_file_path)\n",
        "# Download the file to your local system\n",
        "files.download(profile_file_path)"
      ],
      "metadata": {
        "id": "OoD2scsZcm-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More lightweight Data Exploration with D-Tale\n",
        "\n",
        "Alternatively, D-Tale is a lightweight tool that provides an interactive web-based interface for viewing and analyzing Pandas data structures. It's a great alternative for quick and efficient data exploration without the overhead of more comprehensive tools like `pandas_profiling`.\n",
        "\n",
        "In this section, we'll set up and use D-Tale to explore our dataset.\n"
      ],
      "metadata": {
        "id": "aiu0H1VAlJ6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting D-Tale Session\n",
        "\n",
        "Once D-Tale is installed, we can start a session to view our dataset. After running the code below, you'll receive a link. Clicking on this link will open the D-Tale interface in a new tab, allowing for interactive exploration of the data.\n"
      ],
      "metadata": {
        "id": "QPkAWsB9lyAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dtale\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "# This is necessary in Colab to ensure the D-Tale instance keeps running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Start D-Tale session\n",
        "d = dtale.show(numeric_data)\n",
        "d\n"
      ],
      "metadata": {
        "id": "-icvjdrPlxcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the D-Tale interface, you can:\n",
        "- View the dataset in a tabular format.\n",
        "- Generate charts and visualizations.\n",
        "- Check statistics and distributions of columns.\n",
        "- Run correlations.\n",
        "- And much more!\n",
        "\n",
        "Additionally, D-Tale provides options to export your data or any analysis directly from its interface.\n"
      ],
      "metadata": {
        "id": "rkGoYAPal2aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Corruption Step\n",
        "\n",
        "The data looks pretty clean. That is because it was a high quality dataset imported from Kaggle. In real life, raw data comes with various issues that can hinder or skew our analysis. In this step, we'll intentionally introduce common data problems to our dataset. This will allow us to later demonstrate corrective measures in a practical context.\n",
        "\n",
        "The issues we'll introduce are:\n",
        "- Missing Values\n",
        "- NaN Values\n",
        "- Inconsistencies\n",
        "- Outliers\n",
        "- Duplicates\n",
        "- Incorrect Data Types\n",
        "- Irrelevant Data\n",
        "- Errors or Typos\n",
        "- Biased Data\n",
        "\n",
        "Let's corrupt our data!\n"
      ],
      "metadata": {
        "id": "_SMn6wTyZmps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce Missing Values\n",
        "for col in data.columns:\n",
        "    data.loc[data.sample(frac=0.1).index, col] = None\n",
        "\n",
        "# Introduce NaN Values\n",
        "data.loc[data.sample(frac=0.05).index, 'radius_mean'] = np.nan\n",
        "\n",
        "# Introduce Inconsistencies (using different units or scales)\n",
        "data['texture_mean'] = data['texture_mean'].apply(lambda x: x*10 if random.random() > 0.9 else x)\n",
        "\n",
        "# Introduce Outliers\n",
        "data.loc[data.sample(frac=0.02).index, 'area_mean'] = data['area_mean'].mean() + (data['area_mean'].std() * 10)\n",
        "\n",
        "# Introduce Duplicates\n",
        "duplicates = data.sample(frac=0.05)\n",
        "data = pd.concat([data, duplicates])\n",
        "\n",
        "# Introduce Incorrect Data Types\n",
        "data['id'] = data['id'].astype(str)\n",
        "\n",
        "# Introduce Irrelevant Data (adding a column that doesn't relate to the analysis)\n",
        "data['irrelevant_data'] = [random.choice(['A', 'B', 'C']) for _ in range(len(data))]\n",
        "\n",
        "# Introduce Errors or Typos in 'diagnosis' column\n",
        "data['diagnosis'] = data['diagnosis'].apply(lambda x: 'N' if x == 'M' and random.random() > 0.95 else x)\n",
        "\n",
        "# Display the first few rows of the corrupted data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "2o06sdwCaqyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrective Measures\n",
        "\n",
        "Now that our data is corrupted, let's address each issue step by step. For each problem, we'll provide multiple corrective methods, allowing you to choose the most suitable one based on the specific context of the data.\n"
      ],
      "metadata": {
        "id": "kRsHw9Jla8UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Missing Values\n",
        "missing_value_method = \"median\"  # @param {type:\"string\"} [\"mean\", \"median\", \"mode\", \"drop\"]\n",
        "if missing_value_method == \"mean\":\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "elif missing_value_method == \"median\":\n",
        "    data.fillna(data.median(), inplace=True)\n",
        "elif missing_value_method == \"mode\":\n",
        "    for col in data.columns:\n",
        "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "elif missing_value_method == \"drop\":\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "# Correct NaN Values\n",
        "nan_value_method = \"median\"  # @param {type:\"string\"} [\"mean\", \"median\", \"mode\", \"drop\"]\n",
        "if nan_value_method == \"mean\":\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "elif nan_value_method == \"median\":\n",
        "    data.fillna(data.median(), inplace=True)\n",
        "elif nan_value_method == \"mode\":\n",
        "    for col in data.columns:\n",
        "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "elif nan_value_method == \"drop\":\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "# Correct Inconsistencies\n",
        "# For this example, we'll revert the texture_mean values to their original scale\n",
        "data['texture_mean'] = data['texture_mean'].apply(lambda x: x/10 if x > 100 else x)\n",
        "\n",
        "# Correct Outliers\n",
        "outlier_method = \"Z-Score\"  # @param {type:\"string\"} [\"IQR\", \"Z-Score\", \"drop\"]\n",
        "if outlier_method == \"IQR\":\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "elif outlier_method == \"Z-Score\":\n",
        "    from scipy.stats import zscore\n",
        "    z_scores = zscore(data.select_dtypes(include=[np.number]))\n",
        "    abs_z_scores = np.abs(z_scores)\n",
        "    data = data[(abs_z_scores < 3).all(axis=1)]\n",
        "elif outlier_method == \"drop\":\n",
        "    # Drop rows where 'area_mean' is an outlier\n",
        "    data = data[np.abs(data['area_mean'] - data['area_mean'].mean()) <= (3 * data['area_mean'].std())]\n",
        "\n",
        "# Correct Duplicates\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Correct Incorrect Data Types\n",
        "data['id'] = data['id'].astype(int)\n",
        "\n",
        "# Remove Irrelevant Data\n",
        "data.drop(columns=['irrelevant_data'], inplace=True)\n",
        "\n",
        "# Correct Errors or Typos\n",
        "data['diagnosis'] = data['diagnosis'].apply(lambda x: 'M' if x == 'N' else x)\n",
        "\n",
        "# Display the first few rows of the corrected data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "3gdHD7-ua-oZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}