{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMrGUzTGfOPnIUzeAEb5C9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical_Data_analysis/blob/main/Month_1/Week_1_Data_Importing_and_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analytics with Python: A 6-Month Journey\n",
        "By : [Abderrahim Benmoussa, Ph.D. ](https://https://github.com/wamaw123)\n",
        "\n",
        "Project's on Github : https://github.com/wamaw123/Biomedical_Data_analysis"
      ],
      "metadata": {
        "id": "K6iUFcxHL2oC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1: Data Importing, exploring and Cleaning\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, we'll focus on the foundational steps of any data analysis process:\n",
        "1. **Data Importing**: We'll import a biomedical dataset from a GitHub repository.\n",
        "2. **Descriptive Statistics**: This will give us a preliminary understanding of the dataset's structure and characteristics.\n",
        "3. **Data Cleaning**: We'll handle missing values and outliers to ensure the data's quality.\n",
        "4. **Data Visualization**: Visualizing the data will provide insights into its distribution and potential patterns.\n",
        "5. **Normalization and Standardization**: We'll transform the data to prepare it for future analysis.\n",
        "\n",
        "Let's begin by importing the necessary libraries.\n"
      ],
      "metadata": {
        "id": "FxAjKrnLXnta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas_profiling dtale\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "## Data Manipulation\n",
        "import pandas as pd   # Essential for data manipulation and mathematical operations.\n",
        "import numpy as np    # Used for array-based operations and mathematical functions.\n",
        "\n",
        "## Visualization\n",
        "import matplotlib.pyplot as plt  # Fundamental plotting library.\n",
        "import seaborn as sns            # Builds on top of matplotlib for more advanced visualizations.\n",
        "\n",
        "## Statistical Testing\n",
        "from scipy.stats import shapiro, pearsonr  # Provides functions for statistical testing.\n",
        "\n",
        "## Interactive Exploration\n",
        "import pandas_profiling           # Generates profile reports from a pandas DataFrame.\n",
        "import dtale                      # Interactive tool for data frame exploration.\n",
        "import dtale.app as dtale_app\n",
        "import ipywidgets as widgets      # For creating interactive widgets.\n",
        "from IPython.display import display  # Helps in displaying objects in Jupyter.\n",
        "\n",
        "## Scaling and standardization\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "## Google Colab-specific\n",
        "from google.colab import files    # Specific tools for Google Colab environment.\n",
        "\n",
        "## Database\n",
        "import sqlite3  # Allows interaction with SQLite databases.\n",
        "\n",
        "# Set up the notebook for visualizations\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "bLt69CiJXndE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's retrieve the dataset from GitHub, load it into a DataFrame, and immediately explore its visualization"
      ],
      "metadata": {
        "id": "HigXu_4oE5We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical_Data_analysis/c072fdafc2b2abe4e002f8611f80bcf5fd8366b8/Datasets/Week_1/week_1.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "CMC0ay4xEzs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Dataset\n",
        "\n",
        "This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. These features describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "The 3-dimensional space is described in the following reference: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets,\" Optimization Methods and Software, 1, 1992, 23-34].\n",
        "\n",
        "You can access this dataset from the following sources:\n",
        "\n",
        "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n",
        "- [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?resource=download)\n",
        "- UW CS FTP Server: `ftp.cs.wisc.edu`, Path: `cd math-prog/cpo-dataset/machine-learn/WDBC/`\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "The dataset contains the following attributes:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32) Ten real-valued features computed for each cell nucleus:\n",
        "\n",
        "   a) Radius (mean of distances from center to points on the perimeter)\n",
        "   b) Texture (standard deviation of gray-scale values)\n",
        "   c) Perimeter\n",
        "   d) Area\n",
        "   e) Smoothness (local variation in radius lengths)\n",
        "   f) Compactness (perimeter^2 / area - 1.0)\n",
        "   g) Concavity (severity of concave portions of the contour)\n",
        "   h) Concave points (number of concave portions of the contour)\n",
        "   i) Symmetry\n",
        "   j) Fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "### Class Distribution\n",
        "\n",
        "The class distribution in this dataset is as follows:\n",
        "- 357 benign\n",
        "- 212 malignant"
      ],
      "metadata": {
        "id": "Ss0Ee8rm21ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset can therefore be used for some classification in cancerology. Let's store it into a SQL database to select specific columns to querry. There are many ways to do this without requiring SQL but it is a good occasion to use relational databases for practice."
      ],
      "metadata": {
        "id": "j16Yy8w3Fc2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new SQLite database in memory\n",
        "conn = sqlite3.connect(':memory:')\n",
        "\n",
        "# Load the dataset into the SQLite database\n",
        "data.to_sql('biomedical_data', conn, if_exists='replace')\n"
      ],
      "metadata": {
        "id": "HxrE3Nbr_dLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a tool to manually select columns"
      ],
      "metadata": {
        "id": "jgb1Hn2dGI6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create widgets\n",
        "available_columns = widgets.SelectMultiple(\n",
        "    options=data.columns.tolist(),\n",
        "    description='Available Columns',\n",
        "    layout={'height': '150px', 'width': '400px'},\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "selected_columns = widgets.SelectMultiple(\n",
        "    description='Selected Columns',\n",
        "    layout={'height': '150px', 'width': '400px'},\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Define button click actions\n",
        "def add_columns(b):\n",
        "    for item in available_columns.value:\n",
        "        if item not in selected_columns.options:\n",
        "            selected_columns.options += (item,)\n",
        "\n",
        "    available_columns.options = [item for item in data.columns.tolist() if item not in selected_columns.options]\n",
        "\n",
        "def remove_columns(b):\n",
        "    for item in selected_columns.value:\n",
        "        if item not in available_columns.options:\n",
        "            available_columns.options += (item,)\n",
        "\n",
        "    selected_columns.options = [item for item in data.columns.tolist() if item not in available_columns.options]\n",
        "\n",
        "# Create buttons\n",
        "add_button = widgets.Button(description=\"Add >>\")\n",
        "remove_button = widgets.Button(description=\"<< Remove\")\n",
        "\n",
        "add_button.on_click(add_columns)\n",
        "remove_button.on_click(remove_columns)\n",
        "\n",
        "# Group widgets for column selection\n",
        "left_box = widgets.VBox([available_columns, add_button, remove_button])\n",
        "right_box = widgets.VBox([selected_columns])\n",
        "column_selection_box = widgets.HBox([left_box, right_box])\n",
        "\n",
        "# Global variable to store the validated columns\n",
        "validated_columns = []\n",
        "\n",
        "# Define the function to validate and save selected columns\n",
        "def validate_selection(button):\n",
        "    global validated_columns\n",
        "    validated_columns = list(selected_columns.options)\n",
        "\n",
        "    if not validated_columns:\n",
        "        print(\"No columns selected. Please select columns from the 'Selected Columns' box before validating.\")\n",
        "    else:\n",
        "        print(f\"Selected columns have been saved: {', '.join(validated_columns)}\")\n",
        "\n",
        "# Button to validate and save the selected columns\n",
        "validate_button = widgets.Button(description=\"Validate Selection\")\n",
        "validate_button.on_click(validate_selection)\n",
        "\n",
        "# Group all widgets in a VBox and display\n",
        "all_widgets = widgets.VBox([column_selection_box, validate_button])\n",
        "display(all_widgets)\n"
      ],
      "metadata": {
        "id": "OCmzfrOkAE0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we select the rows on which we want to work. We can keep all of the or just some of them. Less rows will mean less accurate models but for some use cases, it can be good to use less to work the code out."
      ],
      "metadata": {
        "id": "LVWBwYseGxrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a slider for selecting row range\n",
        "row_slider = widgets.IntRangeSlider(\n",
        "    value=[0, len(data)],\n",
        "    min=0,\n",
        "    max=len(data),\n",
        "    step=1,\n",
        "    description='Row Range:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "# Display the row selection slider\n",
        "display(row_slider)\n"
      ],
      "metadata": {
        "id": "9rYFw7rmGl0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally let's querry the data"
      ],
      "metadata": {
        "id": "9IjWwPSlHnve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to store the selected data for subsequent use\n",
        "selected_data = None\n",
        "\n",
        "# Define the function to query data based on user selection\n",
        "def get_selected_data(button):\n",
        "    global selected_data  # Declare the variable as global\n",
        "    columns = validated_columns  # Use the validated_columns global variable\n",
        "\n",
        "    # Check if columns are selected\n",
        "    if not columns:\n",
        "        print(\"Please validate your column selection before querying.\")\n",
        "        return\n",
        "\n",
        "    start_row = row_slider.value[0]\n",
        "    end_row = row_slider.value[1]\n",
        "\n",
        "    # Construct the SQL query with double quotes around column names\n",
        "    columns_str = ', '.join([f'\"{col}\"' for col in columns])\n",
        "    query = f\"SELECT {columns_str} FROM biomedical_data LIMIT {start_row}, {end_row - start_row + 1}\"\n",
        "\n",
        "    result = pd.read_sql_query(query, conn)\n",
        "    selected_data = result  # Assign the result to the global variable\n",
        "    display(result)\n",
        "\n",
        "# Button to execute the query and display results\n",
        "query_button = widgets.Button(description=\"Show Data\")\n",
        "query_button.on_click(get_selected_data)\n",
        "\n",
        "# Display the button\n",
        "display(query_button)\n"
      ],
      "metadata": {
        "id": "Y41TQilNCFZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive Statistics and Data Exploration\n",
        "\n",
        "In this section, we'll delve deep into our dataset to understand its structure, characteristics, and potential issues. This includes understanding basic information, central tendencies, visualizations, and more.\n"
      ],
      "metadata": {
        "id": "TYtpPjFcXyQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first define few usefull functions"
      ],
      "metadata": {
        "id": "CsCibJI-TM2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot histograms for each column in the DataFrame\n",
        "def plot_histograms(df):\n",
        "    \"\"\"\n",
        "    Plot histograms for each column in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    \"\"\"\n",
        "    df.hist(figsize=(20, 15))\n",
        "    plt.tight_layout()  # Adjusts subplot params for better layout\n",
        "    plt.show()\n",
        "\n",
        "# Define a function to plot a heatmap of correlations between columns in the DataFrame\n",
        "def plot_corr_heatmap(df):\n",
        "    \"\"\"\n",
        "    Plot a heatmap of correlations between columns in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "# Define a function to print skewness and kurtosis for each column in the DataFrame\n",
        "def print_skewness_kurtosis(df):\n",
        "    \"\"\"\n",
        "    Print skewness and kurtosis for each column in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    \"\"\"\n",
        "    print(\"\\nSkewness:\\n\", df.skew())\n",
        "    print(\"\\nKurtosis:\\n\", df.kurtosis())\n",
        "\n",
        "\n",
        "# Define a function to pmake violin plots for each column in the DataFrame\n",
        "\n",
        "def plot_violinplots(df):\n",
        "    for col in df.columns:\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        sns.violinplot(y=df[col])\n",
        "        plt.title(f\"Violin plot of {col}\")\n",
        "        plt.show()\n",
        "\n",
        "# Define a function to print missing values information and plot a heatmap of missing values\n",
        "def print_missing_values_info(df):\n",
        "    \"\"\"\n",
        "    Print missing values count and percentage for each column in the DataFrame.\n",
        "    Also, plot a heatmap of missing values.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    \"\"\"\n",
        "    # Print missing values count and percentage\n",
        "    print(\"\\nMissing Values Count:\\n\", df.isnull().sum())\n",
        "    print(\"\\nPercentage of Missing Values:\\n\", (df.isnull().sum() / len(df)) * 100)\n",
        "\n",
        "    # Plot heatmap of missing values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "    plt.title(\"Heatmap of Missing Values (Yellow indicates missing data)\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GLIKsugeS8B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now select numericals data for exploration"
      ],
      "metadata": {
        "id": "4CyY5znrTQJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract only the numeric columns from the selected data for further analysis\n",
        "numeric_selected_data = selected_data.select_dtypes(include=[np.number])"
      ],
      "metadata": {
        "id": "8fKDqz8LTBHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally perform descriptive statistics and check some visualizations to understand our dataset"
      ],
      "metadata": {
        "id": "PnrRsc7TTUXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics\n",
        "print(\"\\nDescriptive Statistics:\\n\", selected_data.describe(include='all'))\n",
        "\n",
        "# Visualizations\n",
        "\n",
        "# Histograms\n",
        "plot_histograms(numeric_selected_data)\n",
        "\n",
        "# Pairplots - activate for small datasets, else it can be very very greedy in compute\n",
        "#sns.pairplot(numeric_selected_data)\n",
        "#plt.suptitle(\"Pair Plot of Numeric Features\", y=1.02)\n",
        "#plt.show()\n",
        "\n",
        "# Violin plots for each numeric column\n",
        "plot_violinplots(numeric_selected_data)\n",
        "\n",
        "# Outlier detection\n",
        "Q1 = numeric_selected_data.quantile(0.25)\n",
        "Q3 = numeric_selected_data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((numeric_selected_data < (Q1 - 1.5 * IQR)) | (numeric_selected_data > (Q3 + 1.5 * IQR))).sum()\n",
        "print(\"\\nOutliers Count (using IQR method):\\n\", outliers)\n",
        "\n",
        "# Call the function to display missing values info and heatmap\n",
        "print_missing_values_info(selected_data)\n",
        "\n",
        "# Correlation Analysis\n",
        "correlations = numeric_selected_data.corr()\n",
        "# Print significant correlations\n",
        "for col in correlations.columns:\n",
        "    for idx in correlations.index:\n",
        "        if idx >= col:  # This avoids duplicate pairs\n",
        "            continue\n",
        "        corr_val = correlations.loc[idx, col]\n",
        "        _, p_value = pearsonr(numeric_selected_data[idx], numeric_selected_data[col])\n",
        "        if p_value < 0.05:\n",
        "            print(f\"Significant correlation between {idx} and {col}: {corr_val:.2f} (p-value: {p_value:.5f})\")\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plot_corr_heatmap(numeric_selected_data)\n",
        "\n",
        "\n",
        "# Skewness and kurtosis\n",
        "print_skewness_kurtosis(numeric_selected_data)"
      ],
      "metadata": {
        "id": "7AQpA57TX1wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does the trick of showing some exploratory information but it is not very user friendly which can be limiting when speaking with people outside the field or not use to colab. It is also fairly long to complete."
      ],
      "metadata": {
        "id": "uBoZA3DRLmJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Data Exploration with pandas_profiling\n",
        "\n",
        " For a more interactive and comprehensive overview of our dataset, we can use the `pandas_profiling` package. This tool generates an interactive HTML report that provides a deep dive into each column, correlations, missing values, and much more.\n",
        "\n",
        "### NOTE : Panda profiling is a powerfull tool but the file can be very large and exploring it can lead to buggy behavior. But on the good side, you only need to run this one time and it will make it easy to explore one set of data outside of colab or any other tool.\n"
      ],
      "metadata": {
        "id": "v-URfgw0cpwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the profile report for the selected columns\n",
        "profile = pandas_profiling.ProfileReport(numeric_selected_data)\n",
        "profile_file_path = \"selected_data_profile_report.html\"\n",
        "profile.to_file(output_file=profile_file_path)\n",
        "\n",
        "# Download the file to your local system\n",
        "files.download(profile_file_path)"
      ],
      "metadata": {
        "id": "OoD2scsZcm-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More light-weight Data Exploration with D-Tale\n",
        "\n",
        "Alternatively, D-Tale is a lightweight tool that provides an interactive web-based interface for viewing and analyzing Pandas data structures. It's a great alternative for quick and efficient data exploration without the overhead of more comprehensive tools like `pandas_profiling`."
      ],
      "metadata": {
        "id": "aiu0H1VAlJ6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting D-Tale Session\n",
        "\n",
        "After running the code below, you'll receive a link. Clicking on this link will open the D-Tale interface in a new tab, allowing for interactive exploration of the data.\n",
        "\n",
        "In the D-Tale interface, you can:\n",
        "- View the dataset in a tabular format.\n",
        "- Generate charts and visualizations.\n",
        "- Check statistics and distributions of columns.\n",
        "- Run correlations.\n",
        "- And much more!\n",
        "\n",
        "Additionally, D-Tale provides options to export your data or any analysis directly from its interface. For now we will just use it to explore visually the dataset and understand it.\n"
      ],
      "metadata": {
        "id": "QPkAWsB9lyAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is necessary in Colab to ensure the D-Tale instance keeps running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Start D-Tale session\n",
        "d = dtale.show(numeric_selected_data)\n",
        "d"
      ],
      "metadata": {
        "id": "-icvjdrPlxcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : if D-tale does not work, one can try using ngrok but ngrok must be setup with tokken already available : see the details on how to do that [here](https://github.com/man-group/dtale#google-colab:~:text=If%20this%20does%20not%20work%20for%20you%20try%20using%20USE_NGROK%20which%20is%20described%20in%20the%20next%20section.)"
      ],
      "metadata": {
        "id": "_lgPyMTxtLjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Automated Data Exploration and Cleaning Tools\n",
        "\n",
        "There are several other tools that one can use to explore the dataset, identify common problems like missing values, inconsistencies, outliers, and more. We will not use them here but they are kept for the record.\n",
        "\n",
        "### 1. [sweetviz](https://github.com/fbdesignpro/sweetviz)\n",
        "An open-source Python library generating beautiful visualizations for EDA. It provides dataset comparisons, target value analysis, and highlights missing and zero values.\n",
        "```python\n",
        "# !pip install sweetviz\n",
        "# import sweetviz as sv\n",
        "# report = sv.analyze(your_dataframe)\n",
        "# report.show_html('report.html')\n",
        "```\n",
        "\n",
        "### 2. [DataPrep.EDA](https://github.com/sfu-db/dataprep)\n",
        "Allows you to explore the dataset with a single line of code, providing insights on missing values, data distribution, correlation, and more.\n",
        "```python\n",
        "# !pip install dataprep\n",
        "# from dataprep.eda import create_report\n",
        "# report = create_report(your_dataframe)\n",
        "# report.show_browser()\n",
        "```\n",
        "\n",
        "### 3. [datacleaner](https://github.com/rhiever/datacleaner)\n",
        "A Python tool that automatically cleans datasets and readies them for analysis. It can handle missing values and incorrect data types.\n",
        "```python\n",
        "# !pip install datacleaner\n",
        "# from datacleaner import autoclean\n",
        "# your_clean_dataframe = autoclean(your_dataframe)\n",
        "```\n",
        "\n",
        "### 4. [pydqc](https://github.com/SauceCat/pydqc)\n",
        "Designed to automatically compare and validate datasets, generating data summaries, missing value statistics, and more.\n",
        "```python\n",
        "# !pip install pydqc\n",
        "# from pydqc.data_summary import distribution_summary_pretty\n",
        "# distribution_summary_pretty(your_dataframe, 'output_directory')\n",
        "```\n",
        "\n",
        "### 5. [Great Expectations](https://github.com/great-expectations/great_expectations)\n",
        "A Python-based open-source library for validating, documenting, and profiling your data, helping maintain data quality.\n",
        "```python\n",
        "# !pip install great_expectations\n",
        "# import great_expectations as ge\n",
        "# your_dataframe_ge = ge.dataset.PandasDataset(your_dataframe)\n",
        "# your_dataframe_ge.expect_column_values_to_not_be_null('column_name')\n",
        "```\n",
        "\n",
        "### 6. Tidy Data\n",
        "The concept of [\"tidy data\"](https://vita.had.co.nz/papers/tidy-data.html) introduced by Hadley Wickham is a standard to structure datasets to facilitate analysis. Ensuring your data is \"tidy\" can help prevent inconsistencies and irrelevant data.\n",
        "\n",
        "Remember to replace `your_dataframe` with the name of your actual dataframe when using these tools.\n",
        "\n"
      ],
      "metadata": {
        "id": "rkGoYAPal2aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Corruption Step\n",
        "\n",
        "The data looks pretty clean. That is because it was a high quality dataset imported from Kaggle. In real life, raw data comes with various issues that can hinder or skew our analysis. In this step, we'll intentionally introduce common data problems to our dataset. This will allow us to later demonstrate corrective measures in a practical context.\n",
        "\n",
        "The issues we'll introduce are:\n",
        "- Missing Values\n",
        "- NaN Values\n",
        "- Inconsistencies\n",
        "- Outliers\n",
        "- Duplicates\n",
        "- Incorrect Data Types\n",
        "- Irrelevant Data\n",
        "- Errors or Typos\n",
        "- Biased Data\n",
        "\n",
        "Let's corrupt our data!\n"
      ],
      "metadata": {
        "id": "_SMn6wTyZmps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce Missing Values\n",
        "for col in selected_data.columns:\n",
        "    selected_data.loc[selected_data.sample(frac=0.1).index, col] = None\n",
        "\n",
        "# Introduce NaN Values\n",
        "selected_data.loc[selected_data.sample(frac=0.05).index, 'radius_mean'] = np.nan\n",
        "\n",
        "# Introduce Inconsistencies (using different units or scales)\n",
        "selected_data['texture_mean'] = selected_data['texture_mean'].apply(lambda x: x*10 if random.random() > 0.9 else x)\n",
        "\n",
        "# Introduce Outliers\n",
        "selected_data.loc[selected_data.sample(frac=0.02).index, 'area_mean'] = selected_data['area_mean'].mean() + (selected_data['area_mean'].std() * 10)\n",
        "\n",
        "# Introduce Duplicates\n",
        "duplicates = selected_data.sample(frac=0.05)\n",
        "selected_data = pd.concat([selected_data, duplicates])\n",
        "\n",
        "# Introduce Incorrect Data Types\n",
        "selected_data['id'] = selected_data['id'].astype(str)\n",
        "\n",
        "# Introduce Irrelevant Data (adding a column that doesn't relate to the analysis)\n",
        "selected_data['irrelevant_data'] = [random.choice(['A', 'B', 'C']) for _ in range(len(selected_data))]\n",
        "\n",
        "# Introduce Errors or Typos in 'diagnosis' column\n",
        "selected_data['diagnosis'] = selected_data['diagnosis'].apply(lambda x: 'N' if x == 'M' and random.random() > 0.95 else x)\n",
        "\n",
        "# Display the first few rows of the corrupted data\n",
        "selected_data.head()\n"
      ],
      "metadata": {
        "id": "2o06sdwCaqyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look again at the dataset now it has been corrupted"
      ],
      "metadata": {
        "id": "mitKKUPG_Bz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to display missing values info and heatmap\n",
        "print_missing_values_info(selected_data)\n",
        "\n",
        "# This is necessary in Colab to ensure the D-Tale instance keeps running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Start D-Tale session\n",
        "d = dtale.show(selected_data)\n",
        "d"
      ],
      "metadata": {
        "id": "MbIP4GmdlNXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrective Measures - Make sure to drop line when diagnosis is NaN\n",
        "\n",
        "Now that our data is corrupted, let's address each issue step by step. For each problem, we'll provide multiple corrective methods, allowing you to choose the most suitable one based on the specific context of the data.\n"
      ],
      "metadata": {
        "id": "kRsHw9Jla8UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_data.head()"
      ],
      "metadata": {
        "id": "dgjNdGqnpuqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = selected_data\n",
        "# Correct Missing Values\n",
        "missing_value_method = \"median\"  # @param {type:\"string\"} [\"mean\", \"median\", \"mode\", \"drop\"]\n",
        "if missing_value_method == \"mean\":\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "elif missing_value_method == \"median\":\n",
        "    data.fillna(data.median(), inplace=True)\n",
        "elif missing_value_method == \"mode\":\n",
        "    for col in data.columns:\n",
        "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "elif missing_value_method == \"drop\":\n",
        "    data.dropna(inplace=True)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "3gdHD7-ua-oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct NaN Values\n",
        "nan_value_method = \"median\"  # @param {type:\"string\"} [\"mean\", \"median\", \"mode\", \"drop\"]\n",
        "if nan_value_method == \"mean\":\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "elif nan_value_method == \"median\":\n",
        "    data.fillna(data.median(), inplace=True)\n",
        "elif nan_value_method == \"mode\":\n",
        "    for col in data.columns:\n",
        "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "elif nan_value_method == \"drop\":\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "data.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "OocGy84kqPkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check for outliers. Be carefull, the Z-score and IQR methods are highly dependant on the distribution."
      ],
      "metadata": {
        "id": "rBh8XZj4rN9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Inconsistencies\n",
        "# For this example, we'll revert the texture_mean values to their original scale\n",
        "data['texture_mean'] = data['texture_mean'].apply(lambda x: x/10 if x > 100 else x)\n",
        "\n",
        "# Correct Outliers\n",
        "outlier_method = \"IQR\"  # @param {type:\"string\"} [\"IQR\", \"Z-Score\", \"drop\"]\n",
        "if outlier_method == \"IQR\":\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "elif outlier_method == \"Z-Score\":\n",
        "    from scipy.stats import zscore\n",
        "    z_scores = zscore(data.select_dtypes(include=[np.number]))\n",
        "    abs_z_scores = np.abs(z_scores)\n",
        "    data = data[(abs_z_scores < 3).all(axis=1)]\n",
        "elif outlier_method == \"drop\":\n",
        "    # Drop rows where 'area_mean' is an outlier\n",
        "    data = data[np.abs(data['area_mean'] - data['area_mean'].mean()) <= (3 * data['area_mean'].std())]\n",
        "\n",
        "data.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "qyqjqai0qiiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Duplicates\n",
        "data.drop_duplicates(inplace=True)\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "fDq79pDzqr8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the lines where id is nan or \"nan\"\n",
        "data.dropna(subset=['id'], inplace=True)\n",
        "data = data[data['id'] != 'nan']\n",
        "#Drop the lines where diagnosis is nan since it is the targeted variable\n",
        "data.dropna(subset=['diagnosis'], inplace=True)\n",
        "data = data[data['diagnosis'] != 'nan']\n",
        "\n",
        "data.head()\n",
        "data.describe"
      ],
      "metadata": {
        "id": "pRZHzGVqrmfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Incorrect Data Types\n",
        "data['id'] = data['id'].astype(float).astype(int)\n",
        "data['id'] = data['id'].astype(int)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "iy3OK0NEqtVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create widgets\n",
        "available_columns = widgets.SelectMultiple(\n",
        "    options=data.columns.tolist(),\n",
        "    description='Available Columns',\n",
        "    layout={'height': '150px', 'width': '400px'},\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "selected_columns = widgets.SelectMultiple(\n",
        "    description='Columns to Drop',\n",
        "    layout={'height': '150px', 'width': '400px'},\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Define button click actions\n",
        "def add_columns(b):\n",
        "    for item in available_columns.value:\n",
        "        if item not in selected_columns.options:\n",
        "            selected_columns.options += (item,)\n",
        "\n",
        "    available_columns.options = [item for item in data.columns.tolist() if item not in selected_columns.options]\n",
        "\n",
        "def remove_columns(b):\n",
        "    for item in selected_columns.value:\n",
        "        if item not in available_columns.options:\n",
        "            available_columns.options += (item,)\n",
        "\n",
        "    selected_columns.options = [item for item in data.columns.tolist() if item not in available_columns.options]\n",
        "\n",
        "# Create buttons\n",
        "add_button = widgets.Button(description=\"Add >>\")\n",
        "remove_button = widgets.Button(description=\"<< Remove\")\n",
        "\n",
        "add_button.on_click(add_columns)\n",
        "remove_button.on_click(remove_columns)\n",
        "\n",
        "# Group widgets for column selection\n",
        "left_box = widgets.VBox([available_columns, add_button, remove_button])\n",
        "right_box = widgets.VBox([selected_columns])\n",
        "column_selection_box = widgets.HBox([left_box, right_box])\n",
        "\n",
        "# Global variable to store the columns to be dropped\n",
        "columns_to_drop = []\n",
        "\n",
        "# Define the function to validate and save columns to drop\n",
        "def validate_selection(button):\n",
        "    global columns_to_drop\n",
        "    columns_to_drop = list(selected_columns.options)\n",
        "\n",
        "    if not columns_to_drop:\n",
        "        print(\"No columns selected to drop. Please select columns from the 'Columns to Drop' box before validating.\")\n",
        "    else:\n",
        "        print(f\"Columns selected to be dropped: {', '.join(columns_to_drop)}\")\n",
        "        data.drop(columns=columns_to_drop, inplace=True)  # This line drops the selected columns from the DataFrame\n",
        "        print(\"Columns have been dropped from the DataFrame!\")\n",
        "\n",
        "# Button to validate and save the columns to drop\n",
        "validate_button = widgets.Button(description=\"Drop Selected Columns\")\n",
        "validate_button.on_click(validate_selection)\n",
        "\n",
        "# Group all widgets in a VBox and display\n",
        "all_widgets = widgets.VBox([column_selection_box, validate_button])\n",
        "display(all_widgets)\n"
      ],
      "metadata": {
        "id": "u5uhWCpbqu4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct Errors or Typos\n",
        "data['diagnosis'] = data['diagnosis'].apply(lambda x: 'M' if x == 'N' else x)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "mwID2jRXqxle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can also directly solve some of these issues in D-Tale, export as CSV and then open back in panda to solve the rest (drop NaN etc)."
      ],
      "metadata": {
        "id": "SoFtVtkunn7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization and Standardization\n",
        "\n",
        "Finally, we'll transform our data to ensure it's on a consistent scale. This is crucial for many machine learning algorithms. We'll use:\n",
        "1. **Min-Max Normalization**: This scales the data between 0 and 1.\n",
        "2. **Z-score Standardization**: This scales the data based on its mean and standard deviation.\n"
      ],
      "metadata": {
        "id": "6UsNB4QLtySY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(data)\n",
        "\n",
        "# Select only numerical columns (excluding 'diagnosis' and 'id')\n",
        "num_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "num_cols = num_cols.drop(['id'])  # exclude id column\n",
        "\n",
        "if len(num_cols) > 0:\n",
        "    # Normalize\n",
        "    scaler_norm = MinMaxScaler()\n",
        "    data_normalized = data.copy()\n",
        "    data_normalized[num_cols] = scaler_norm.fit_transform(data[num_cols])\n",
        "    normalized_data = data_normalized\n",
        "\n",
        "    # Standardize\n",
        "    scaler_std = StandardScaler()\n",
        "    data_standardized = data.copy()\n",
        "    data_standardized[num_cols] = scaler_std.fit_transform(data[num_cols])\n",
        "    standardized_data = data_standardized\n",
        "\n",
        "    print(\"\\nNormalized Data:\")\n",
        "    print(normalized_data.head())\n",
        "    print(\"\\nStandardized Data:\")\n",
        "    print(standardized_data.head())\n",
        "else:\n",
        "    print(\"There are no numerical columns to normalize or standardize.\")\n"
      ],
      "metadata": {
        "id": "X5IdWiYyt5bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQ5XK739x-tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There ! We have somewhat proper dataset, ready to use for modeling or statistical analyses."
      ],
      "metadata": {
        "id": "iUbwIYrQx1FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to display missing values info and heatmap\n",
        "print_missing_values_info(normalized_data)\n",
        "\n",
        "#Duplicates\n",
        "print(\"\\nNumber of duplicate rows:\", normalized_data.duplicated().sum())\n",
        "normalized_data.drop_duplicates(inplace=True)\n",
        "print(\"Duplicates removed. New shape:\", normalized_data.shape)\n",
        "\n",
        "# This is necessary in Colab to ensure the D-Tale instance keeps running\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Start D-Tale session\n",
        "d = dtale.show(normalized_data)\n",
        "d"
      ],
      "metadata": {
        "id": "k6ygJU8ZyCu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and perspectives\n",
        "\n",
        "Analyzing and preprocessing data is a pivotal aspect of data science. While it can be time-consuming, meticulous and rigorous attention to this phase can significantly enhance model performance.\n",
        "\n",
        "Several strategies exist to address the issues mentioned. Although I've highlighted traditional methods, it's essential to remember that tasks like data cleaning, imputation, and normalization should be undertaken in collaboration with subject matter experts, examining each feature individually and choosing the proper strategy. In some occasions, droping features might be more advisable than droping lines to ensure accuracy in the later modeling steps.\n",
        "\n",
        "Few things to improve in this document for proper data-handling :\n",
        "- It could be better practice to create new dataframes at each steps instead of working on the entire dataframes. This would ensure data integrity and facilitates easier tracking of changes.\n",
        "- One might want to run a full diagnosis on the data once cleaned and loop the error solving issues\n",
        "- Visual inspection of the data might also help spot some errors that are comon and one might use regex to solve some typos and common data errors and insertions (unwanted spaces, underlines, comas instead of points, etc)\n",
        "- A lot of code here is more for user experience than really for the targeted objective. Depending on the project, it might be better to keep the code tidy or more user friendly\n",
        "- The code might need to be tidyer\n",
        "- Some functions would probably be better in a method file ajuncted to this document in a normal workflow. Here, I chose to keep everything in one document for clarity.\n",
        "- Some of the comments and document description might beneficiate of a better formulation\n",
        "- A link to D-Tale or other tools tutorials might be a good idea to add."
      ],
      "metadata": {
        "id": "ZdpByMaitLpV"
      }
    }
  ]
}