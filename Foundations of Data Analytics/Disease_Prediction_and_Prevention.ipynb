{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical-Data-Analytics-with-Python/blob/main/Foundations%20of%20Data%20Analytics/Disease_Prediction_and_Prevention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i8SP3ReLr0I"
      },
      "source": [
        "# Data Analytics with Python\n",
        "By : [Abderrahim Benmoussa, Ph.D. ](https://https://github.com/wamaw123)\n",
        "\n",
        "Project's on Github : https://github.com/wamaw123/Biomedical_Data_analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33r-b9O2cBwK"
      },
      "source": [
        "# Disease Prediction and Prevention\n",
        "\n",
        "In this notebook, we aim to predict the 10-year risk of future coronary heart disease (CHD) using the Framingham Heart Study dataset. This will be a binary classification task, where `1` indicates the risk of CHD, and `0` indicates no risk. To do so we will explore the dataset and go on to test a simple logistic regression model first. We will then compare different models and finally optimize the model to get the best predictive accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install and Import Libraries\n",
        "\n",
        "In this step, we will install and import necessary libraries for our analysis.\n",
        "- `pandas` and `numpy` for data manipulation\n",
        "- `seaborn` and `matplotlib` for data visualization\n",
        "- `scikit-learn` for building and evaluating the machine learning model"
      ],
      "metadata": {
        "id": "iv0Svz3JoO4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIbD25DnbYKc"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas numpy scipy statsmodels patsy dtale scikit-learn pandas_profiling pycaret\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "## Data Manipulation\n",
        "import pandas as pd   # Essential for data manipulation and mathematical operations.\n",
        "import numpy as np    # Used for array-based operations and mathematical functions.\n",
        "\n",
        "## Visualization\n",
        "import matplotlib.pyplot as plt  # Fundamental plotting library.\n",
        "import seaborn as sns            # Builds on top of matplotlib for more advanced visualizations.\n",
        "\n",
        "## Statistical Testing, modeling and data preparation\n",
        "from scipy import stats           # Library for scientific and technical computing.\n",
        "import statsmodels.api as sm      # Provides classes and functions for the estimation of many different statistical models.\n",
        "import statsmodels.formula.api as smf  # Formula-based API for the statsmodels library.\n",
        "from sklearn.model_selection import train_test_split  # Import train_test_split function to split data into training and testing sets.\n",
        "from sklearn.preprocessing import StandardScaler  # Import StandardScaler to standardize features by removing the mean and scaling to unit variance.\n",
        "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression to perform logistic regression.\n",
        "from sklearn.metrics import classification_report, accuracy_score  # Import classification_report to build a text report showing the main classification metrics, and accuracy_score to compute the accuracy of the algorithm.\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "## Interactive Exploration\n",
        "from collections import Counter\n",
        "import pandas_profiling as pp\n",
        "\n",
        "#Automated Ml\n",
        "from pycaret.classification import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXnchmZHfu8E"
      },
      "source": [
        "## Step 2: Load the Dataset\n",
        "\n",
        "Next we load the week 2 dataset directly from GitHub and set it into a Pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdQjhLwPf5ZH"
      },
      "outputs": [],
      "source": [
        "# Fetch the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical-Data-Analytics-with-Python/afab193c5cb3d6878755c4d12e8baa821a8ab054/Datasets/23/framingham.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UraLwWhrfXQ6"
      },
      "source": [
        "#About the dataset\n",
        "\n",
        "## Source\n",
        "The dataset is publicly available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD). The dataset provides the patientsâ€™ information. It includes over 4,000 records and 15 attributes.\n",
        "\n",
        "## Variables\n",
        "Each attribute is a potential risk factor. There are both demographic, behavioral, and medical risk factors.\n",
        "\n",
        "### Demographic:\n",
        "- **Sex:** male or female (Nominal)\n",
        "- **Age:** Age of the patient; (Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
        "\n",
        "### Behavioral:\n",
        "- **Current Smoker:** whether or not the patient is a current smoker (Nominal)\n",
        "- **Cigs Per Day:** the number of cigarettes that the person smoked on average in one day. (Can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
        "\n",
        "### Medical (history):\n",
        "- **BP Meds:** whether or not the patient was on blood pressure medication (Nominal)\n",
        "- **Prevalent Stroke:** whether or not the patient had previously had a stroke (Nominal)\n",
        "- **Prevalent Hyp:** whether or not the patient was hypertensive (Nominal)\n",
        "- **Diabetes:** whether or not the patient had diabetes (Nominal)\n",
        "\n",
        "### Medical (current):\n",
        "- **Tot Chol:** total cholesterol level (Continuous)\n",
        "- **Sys BP:** systolic blood pressure (Continuous)\n",
        "- **Dia BP:** diastolic blood pressure (Continuous)\n",
        "- **BMI:** Body Mass Index (Continuous)\n",
        "- **Heart Rate:** heart rate (Continuous - In medical research, variables such as heart rate, though in fact discrete, yet are considered continuous because of a large number of possible values.)\n",
        "- **Glucose:** glucose level (Continuous)\n",
        "\n",
        "### Predict variable (desired target):\n",
        "- **10-year risk of coronary heart disease CHD (binary:** \"1\" means \"Yes,\" \"0\" means \"No\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Preprocessing\n",
        "\n",
        "We will perform initial data preprocessing such as handling missing values. This step is crucial to ensure the quality and reliability of our machine learning model. There are many ways to deal with missing values for instance. Those can be droped or inputed in different ways. Let's first check what missing values we have."
      ],
      "metadata": {
        "id": "tqApg3NKos5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "# isnull() returns a DataFrame where each cell is either True or False depending on that cell's null status.\n",
        "# sum() will then sum the True values (count of missing values) for each column.\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "# Display the percentage of missing values for each column\n",
        "print(f\"Percentage of missing values for each column:\\n{missing_percentage}\")\n",
        "\n",
        "# Visualize the missing values as a heatmap\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "sns.heatmap(df.isnull(),     # Provide DataFrame with null-status information\n",
        "            cbar=False,      # Do not draw a color bar\n",
        "            cmap='viridis')  # Use the viridis color map\n",
        "\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gY69h8Dvozmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : Education and the values for glycemia seem to be the ones with most missing values. There are no missing values for the 10 year CHD. There are many ways to deal with missing values.\n",
        "\n",
        "- Remove Rows with Missing Values\n",
        "- Replace Missing Values with Mean\n",
        "- Replace Missing Values with Median\n",
        "- Replace Missing Values with Mode\n",
        "- Use Forward or Backward Fill\n",
        "\n",
        "I would prefer either dropping the values since only 10% are missing for glycemia or using median. I don't expect much difference between the two, so I will go with either of them."
      ],
      "metadata": {
        "id": "hXYd69j5s2gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values based on user's choice\n",
        "def handle_missing_values(df, option):\n",
        "    \"\"\"Handle missing values based on user's choice.\"\"\"\n",
        "    if option == 'Remove Rows':\n",
        "        df.dropna(inplace=True)\n",
        "    elif option == 'Replace with Mean':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].mean(), inplace=True)\n",
        "    elif option == 'Replace with Median':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].median(), inplace=True)\n",
        "    elif option == 'Replace with Mode':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "    elif option == 'Forward or Backward Fill':\n",
        "        df.fillna(method='ffill', inplace=True)\n",
        "        df.fillna(method='bfill', inplace=True)\n",
        "    return df\n",
        "\n",
        "# User's choice using Google Colab form field dropdown\n",
        "missing_value_option = 'Replace with Median' #@param [\"Remove Rows\", \"Replace with Mean\", \"Replace with Median\", \"Replace with Mode\", \"Forward or Backward Fill\"]\n",
        "\n",
        "# Handle missing values based on user's choice\n",
        "df_nm = handle_missing_values(df, missing_value_option)\n"
      ],
      "metadata": {
        "id": "esguogxLtzbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check again for missing values"
      ],
      "metadata": {
        "id": "pUfbgBsVu1Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "# isnull() returns a DataFrame where each cell is either True or False depending on that cell's null status.\n",
        "# sum() will then sum the True values (count of missing values) for each column.\n",
        "print(df_nm.isnull().sum())\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = (df_nm.isnull().sum() / len(df_nm)) * 100\n",
        "\n",
        "# Display the percentage of missing values for each column\n",
        "print(f\"Percentage of missing values for each column:\\n{missing_percentage}\")\n",
        "\n",
        "# Visualize the missing values as a heatmap\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "sns.heatmap(df_nm.isnull(),     # Provide DataFrame with null-status information\n",
        "            cbar=False,      # Do not draw a color bar\n",
        "            cmap='viridis')  # Use the viridis color map\n",
        "\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HHwdQohFu3Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : The data is now devoid of missing values."
      ],
      "metadata": {
        "id": "6EoDo49Gvm4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now check for correlations between these variables"
      ],
      "metadata": {
        "id": "ZCJHOdPpsl7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(12, 8))  # Set the size of the figure\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,  # Annotate each cell with the numeric value\n",
        "            cmap='coolwarm',  # Use a cool-warm color map\n",
        "            vmin=-1, vmax=1,  # Set color scale limits\n",
        "            linewidths=.5)  # Set linewidth between entries in matrix\n",
        "\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6W4KF0pwspkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjiuuE63hxuh"
      },
      "source": [
        "Analysis of the output : we see some interesting correlations with our dependant variable (target). Mostly age and hypertension hallmarks, which is fairly expectable from a scientific point of view."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Exploratory Data Analysis (EDA) and\n",
        "\n",
        "Let's explore the dataset to understand it better and figure out how to approach the prediction problem. Visualization helps in identifying patterns and anomalies in the dataset. It will be crucial to deal with imbalanced dataset issues by using techniques like oversampling, undersampling, or SMOTE.\n"
      ],
      "metadata": {
        "id": "ikxKynFio4qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting information on the data form\n",
        "df_nm.info()"
      ],
      "metadata": {
        "id": "ieRMvXyEo5ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output interpretation: The table contains 4,238 entries (or rows) and 16 different categories (or columns) of information. These categories include things like gender (male), age, education, whether the person is a current smoker, and the number of cigarettes smoked per day, among others. All entries in the table are non-null, meaning they all contain data, and the data is in different formats, including integers (int64) and floating-point numbers (float64). The table takes up about 530 kilobytes of memory space."
      ],
      "metadata": {
        "id": "Qm2xvU4wxN_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's further explore the data using panda profiling"
      ],
      "metadata": {
        "id": "sIRq6RZryuBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp.ProfileReport(df_nm)"
      ],
      "metadata": {
        "id": "wdse1msvy5e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall informations gathered from Panda Profiling :\n",
        "\n",
        "- There's a strong relationship between the number of cigarettes smoked per day (cigsPerDay) and whether the person is a current smoker (currentSmoker).\n",
        "- Systolic blood pressure (sysBP) is closely related to diastolic blood pressure (diaBP) and one other field.\n",
        "- Diastolic blood pressure (diaBP) is also closely related to systolic blood pressure (sysBP) and one other field.\n",
        "- There's a strong link between glucose levels and diabetes.\n",
        "- PrevalentHyp is strongly connected with sysBP and one other field.\n",
        "- The BPMeds, prevalentStroke, and diabetes fields are highly imbalanced, meaning most of the values are the same (80.8% for BPMeds, 94.8% for prevalentStroke, and 82.8% for diabetes).\n",
        "- Half of the cigsPerDay entries are zero, indicating a lot of non-smokers or missing data : in this case, probably non-smokers since missing data were initial NaNs."
      ],
      "metadata": {
        "id": "UXlnX_U2zVDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imbalance in Data and Why It's an Issue\n",
        "\n",
        "\"Imbalance\" in data refers to a situation where the distribution of data among different categories or classes is unequal. One class may have significantly more instances than others.\n",
        "\n",
        "## Issues Caused by Imbalance:\n",
        "\n",
        "1. **Model Bias:**\n",
        "   - An imbalanced dataset can cause a predictive model to be biased towards the majority class, resulting in poor performance for the minority class.\n",
        "\n",
        "2. **Misleading Accuracy:**\n",
        "   - A model might show high accuracy by simply predicting the majority class, providing a false sense of effectiveness.\n",
        "\n",
        "3. **Loss of Information:**\n",
        "   - Patterns associated with the minority class may be overlooked, leading to a lack of important insights.\n",
        "\n",
        "4. **Overfitting:**\n",
        "   - The model may memorize the few available minority class instances rather than generalizing, leading to overfitting.\n",
        "\n",
        "5. **False Assumptions:**\n",
        "   - Incorrect assumptions may be made about real-world class distributions, affecting model performance in practical applications.\n",
        "\n",
        "## Solutions:\n",
        "\n",
        "1. **Resampling:**\n",
        "   - Oversample the minority class or undersample the majority class.\n",
        "\n",
        "2. **Synthetic Data Generation:**\n",
        "   - Generate new data points for the minority class, e.g., using the Synthetic Minority Over-sampling Technique (SMOTE).\n",
        "\n",
        "3. **Cost-sensitive Learning:**\n",
        "   - Penalize the misclassification of the minority class more heavily.\n",
        "\n",
        "4. **Using Different Evaluation Metrics:**\n",
        "   - Utilize metrics like the F1-score, precision, recall, or Area Under the Receiver Operating Characteristic (ROC) curve to evaluate model performance.\n",
        "\n",
        "For the sake of this excercise, let's explore resampling and more specifically SMOTE.\n",
        "\n"
      ],
      "metadata": {
        "id": "_LcmhNPz0380"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Necessary Libraries\n",
        "\n",
        "# Assumed data loading\n",
        "# df = pd.read_csv('link_to_dataset')\n",
        "\n",
        "df_rdy = df_nm.copy()\n",
        "\n",
        "# Function to handle imbalance\n",
        "def handle_imbalance(df, column, method):\n",
        "    if method == \"Oversampling\":\n",
        "        df_majority = df[df[column]==0]\n",
        "        df_minority = df[df[column]==1]\n",
        "        df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=123)\n",
        "        df = pd.concat([df_majority, df_minority_upsampled])\n",
        "    elif method == \"Undersampling\":\n",
        "        df_majority = df[df[column]==0]\n",
        "        df_minority = df[df[column]==1]\n",
        "        df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=123)\n",
        "        df = pd.concat([df_minority, df_majority_downsampled])\n",
        "    elif method == \"SMOTE\":\n",
        "        X = df.drop(columns=[column])\n",
        "        y = df[column]\n",
        "        smote = SMOTE(random_state=123)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "        df = pd.concat([X, y], axis=1)\n",
        "    return df\n",
        "\n",
        "# Choice of method to handle imbalance for BPMeds\n",
        "method_BPMeds = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'BPMeds', method_BPMeds)\n",
        "\n",
        "# Choice of method to handle imbalance for prevalentStroke\n",
        "method_prevalentStroke = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'prevalentStroke', method_prevalentStroke)\n",
        "\n",
        "# Choice of method to handle imbalance for diabetes\n",
        "method_diabetes = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'diabetes', method_diabetes)\n",
        "\n",
        "df_rdy.head()"
      ],
      "metadata": {
        "id": "m44EZNNj3N2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore again the dataset now that those issues have been adressed"
      ],
      "metadata": {
        "id": "By3AsBVL332z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp.ProfileReport(df_rdy)"
      ],
      "metadata": {
        "id": "Ic379mkp39Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output analysis : The imbalance issues have been solved but there might be other issues that arose. We can try to work those out later (e.g. drop the diabetes variable since it is uniform, remove some features that are highly correlated with each other to reduce multicollinearity), if they end-up being an issue for modelisation. I put the commented code to do so bellow."
      ],
      "metadata": {
        "id": "f0WlKWtE5JKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "#corr_matrix = df_rdy.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "#upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.8\n",
        "#to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
        "\n",
        "# Drop the highly correlated features\n",
        "#df_rdy = df_rdy.drop(df_rdy[to_drop], axis=1)\n",
        "\n",
        "# Converting 'cigsPerDay' and 'BPMeds' to binary\n",
        "#df_rdy['cigsPerDay'] = df_rdy['cigsPerDay'].apply(lambda x: 1 if x > 0 else 0)\n",
        "#df_rdy['BPMeds'] = df_rdy['BPMeds'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Optionally drop the 'diabetes' column\n",
        "# df_rdy = df_rdy.drop(columns=['diabetes'])\n"
      ],
      "metadata": {
        "id": "AWt8Gn1G6cE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Step 5: Feature Selection and Splitting the Dataset\n",
        "\n",
        "In this step, we will select the relevant features for our model and split the dataset into training and testing sets. This separation allows us to evaluate the model's performance on unseen data.  "
      ],
      "metadata": {
        "id": "HG-sSDp2pG0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting features and target variable\n",
        "X = df_rdy.drop(columns=['TenYearCHD'])\n",
        "y = df_rdy['TenYearCHD']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "Bhw1qTWMpKZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify if there is any imbalance for target variable before going forward"
      ],
      "metadata": {
        "id": "v_a7fx5P7_kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test.unique())\n",
        "Counter(y_train)"
      ],
      "metadata": {
        "id": "ubKWmjYB8BPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a class imbalance in the training dataset: there are significantly more cases of class 0 (18336) than class 1 (7516). This class imbalance can potentially bias the machine learning model towards predicting the majority class (0 in this case), as the model will learn that it can achieve a high accuracy by always predicting the majority class. Let's solve it using one of the same approaches we used before but only on the training dataset. Indeed, applying resampling techniques on the entire dataset and then splitting it could cause data leakage, where information from the testing set leaks into the training set. So we will first split the data into training and testing sets, and then apply the resampling techniques only on the training set."
      ],
      "metadata": {
        "id": "MQrYN4mg8MEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle imbalance\n",
        "def handle_imbalance2(X, y, method):\n",
        "    if method == \"Oversampling\":\n",
        "        df = pd.concat([X, y], axis=1)\n",
        "        df_majority = df[df.TenYearCHD == 0]\n",
        "        df_minority = df[df.TenYearCHD == 1]\n",
        "        df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=123)\n",
        "        df = pd.concat([df_majority, df_minority_upsampled])\n",
        "        X = df.drop(columns=['TenYearCHD'])\n",
        "        y = df['TenYearCHD']\n",
        "    elif method == \"Undersampling\":\n",
        "        df = pd.concat([X, y], axis=1)\n",
        "        df_majority = df[df.TenYearCHD == 0]\n",
        "        df_minority = df[df.TenYearCHD == 1]\n",
        "        df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=123)\n",
        "        df = pd.concat([df_minority, df_majority_downsampled])\n",
        "        X = df.drop(columns=['TenYearCHD'])\n",
        "        y = df['TenYearCHD']\n",
        "    elif method == \"SMOTE\":\n",
        "        smote = SMOTE(random_state=123)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "    return X, y\n",
        "\n",
        "# Choice of method to handle imbalance\n",
        "method_unbalvar = 'SMOTE'  # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "X_train, y_train = handle_imbalance2(X_train, y_train, method_unbalvar)\n",
        "\n",
        "# Verify the new class distributions\n",
        "print(pd.value_counts(y_train))\n"
      ],
      "metadata": {
        "id": "wOfwx8DH9Uln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The imbalance issue seems to be solved, we can move forward."
      ],
      "metadata": {
        "id": "Vls_omsH_Obf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Feature Scaling\n",
        "\n",
        "Standardizing the dataset is crucial for many machine learning models and helps the model to converge faster.\n"
      ],
      "metadata": {
        "id": "AkJL4Hh8pK9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "gEKUQhFGpM53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 7: Model Building\n",
        "\n",
        "We will use first the Logistic Regression model for prediction, which is a commonly used algorithm for binary classification problems.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AJgB1vNqpNXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "y9Co564OpQYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Model Evaluation\n",
        "\n",
        "Evaluate the model performance by comparing the predicted and actual values. We will use metrics such as precision, recall, and accuracy to assess the model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "-4l8IdakpQu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
      ],
      "metadata": {
        "id": "0PbsIGpzpShw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mdeo Train Score is : ' , model.score(X_train, y_train))\n",
        "print('Model Test Score is : ' , model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "EQ_a0RlTLz3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix"
      ],
      "metadata": {
        "id": "JWOMxMm-LM1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Creating the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizing the confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rPEy1ZytLQcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Output analysis :\n",
        "1. **Precision:**\n",
        "   - Class 0: \\(0.88\\): When the model predicts class 0 (no disorder), it is correct \\(88\\%\\) of the time.\n",
        "   - Class 1: \\(0.60\\): When the model predicts class 1 (CM disorder), it is correct \\(60\\%\\) of the time.\n",
        "   - This tells us about the correctness of the model when making a positive prediction.\n",
        "\n",
        "2. **Recall:**\n",
        "   - Class 0: \\(0.79\\): Of the actual class 0 instances, the model correctly predicted \\(79\\%\\) of them.\n",
        "   - Class 1: \\(0.75\\): Of the actual class 1 instances, the model correctly predicted \\(75\\%\\) of them.\n",
        "   - This metric tells us about the model's ability to identify all the relevant cases within a dataset.\n",
        "\n",
        "3. **F1-Score:**\n",
        "   - Class 0: \\(0.83\\): The harmonic mean of precision and recall for class 0 is \\(83\\%\\).\n",
        "   - Class 1: \\(0.66\\): The harmonic mean of precision and recall for class 1 is \\(66\\%\\).\n",
        "   - The F1 Score is a good way to summarize the evaluation of the model with a single number, with 1 being perfect and 0 being the worst.\n",
        "\n",
        "4. **Support:**\n",
        "   - Class 0: \\(4547\\): There are \\(4547\\) instances of class 0 in the test set.\n",
        "   - Class 1: \\(1917\\): There are \\(1917\\) instances of class 1 in the test set.\n",
        "   - This gives us an idea about the distribution of classes in the test set.\n",
        "\n",
        "5. **Accuracy:**\n",
        "   - \\(0.7767\\) or \\(77.67\\%\\) is the overall accuracy of the model. This means that \\(77.67\\%\\) of all the predictions were correct.\n",
        "\n",
        "6. **Macro Avg:**\n",
        "   - Precision, recall, and F1-score averaged for both classes, without considering the imbalance of classes.\n",
        "   - Precision: \\(0.74\\), Recall: \\(0.77\\), F1-Score: \\(0.75\\).\n",
        "\n",
        "7. **Weighted Avg:**\n",
        "   - Average of the metric, with consideration of class imbalance.\n",
        "   - Precision: \\(0.80\\), Recall: \\(0.78\\), F1-Score: \\(0.78\\).\n",
        "\n",
        "So :\n",
        "\n",
        "- The model performs relatively well, with an overall accuracy of approximately \\(77.67\\%\\).\n",
        "- The model has higher precision for class 0 compared to class 1, which might be an indication that the model is more reliable when predicting class 0.\n",
        "- The recall for both classes is fairly comparable, suggesting the model has a decent ability to identify positive instances for both classes.\n",
        "- There is a significant difference in the F1-score for class 0 and class 1, showing the model has a better balance of precision and recall for class 0.\n",
        "\n",
        "This is a good start but we surely do better."
      ],
      "metadata": {
        "id": "CeWYeh8U_6pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Model optimization"
      ],
      "metadata": {
        "id": "2VDVEY9bCUar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model optimization is the process of finding the best model parameters (hyperparameters) for a given machine learning algorithm to improve its performance. The process involves training the model on different parameter values and evaluating its performance to find the most suitable set of parameters. Different strategies for model optimization include Grid Search, Random Search, and Bayesian Optimization.\n",
        "\n",
        "### What we will do :\n",
        "\n",
        "1. **Selection of Optimization Method**:\n",
        "    - Choose the optimization technique you want to try (Grid Search, Random Search, or Bayesian Optimization). Each method requires different setups and performs the optimization differently:\n",
        "      - Grid Search: Exhaustively searches over a specified parameter grid.\n",
        "      - Random Search: Randomly searches over parameters.\n",
        "      - Bayesian Optimization: Uses probabilistic models to find the minimum of a function.\n",
        "   \n",
        "2. **Setting Model Parameters**:\n",
        "    - Set different hyperparameters for the Logistic Regression model. These include `penalty`, `C`, `solver`, and `max_iter` (see below for details).\n",
        "\n",
        "3. **Fitting the Model**:\n",
        "   - Depending on the chosen method, the model will be fitted with different parameters.\n",
        "      - In Grid Search and Random Search, `GridSearchCV` and `RandomizedSearchCV` are used from `sklearn`.\n",
        "      - In Bayesian Optimization, `BayesianOptimization` from `bayes_opt` is used.\n",
        "   \n",
        "4. **Evaluation**:\n",
        "    - The best model is evaluated on the test set, and the classification report and accuracy are printed.\n",
        "\n",
        "### How to Select Parameters:\n",
        "\n",
        "1. **`penalty`** (regularization term):\n",
        "   - Options: `l1`, `l2`, `elasticnet`, `none`.\n",
        "   - Selection guide: Choose `l1` or `l2` for small datasets and `elasticnet` for larger datasets or when you need to use both `l1` and `l2` regularization.\n",
        "  \n",
        "2. **`C`** (Inverse of regularization strength):\n",
        "   - Options: Continuous values, e.g., 1.0.\n",
        "   - Selection guide: Smaller values specify stronger regularization. Choose values close to zero for strong regularization and higher values for weaker regularization.\n",
        "  \n",
        "3. **`solver`** (algorithm to use in the optimization problem):\n",
        "   - Options: `newton-cg`, `lbfgs`, `liblinear`, `sag`, `saga`.\n",
        "   - Selection guide: For small datasets, `liblinear` is a good choice, whereas `sag` and `saga` are faster for large datasets. If you chose elasticnet before, you need to chose saga.\n",
        "  \n",
        "4. **`max_iter`** (maximum number of iterations taken for the solvers to converge):\n",
        "   - Options: Integer values, e.g., 100.\n",
        "   - Selection guide: For faster convergence, use higher values. Monitor the convergence of the model and increase the value if needed.\n",
        "\n",
        "### Further Tips:\n",
        "\n",
        "- For `Grid Search`, define a grid for each parameter. The algorithm will evaluate the model performance for each combination of parameters.\n",
        "- For `Random Search`, define the distribution for each parameter. The algorithm will randomly sample parameters from the distribution.\n",
        "- For `Bayesian Optimization`, define the range for each parameter. The algorithm will use Bayesian inference to propose better parameters iteratively.\n",
        "  \n",
        "In practice, start with a coarse grid or a broader range and then refine the search space based on initial results for more precise optimization."
      ],
      "metadata": {
        "id": "WRbKkeIhDD5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MANUAL SETTINGS"
      ],
      "metadata": {
        "id": "n-X5sC4pOV2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Form fields for user input\n",
        "penalty = 'l1'  # @param ['l1', 'l2', 'elasticnet', 'none']\n",
        "C = 33  # @param {type: \"number\"}\n",
        "\n",
        "# Setting solver based on penalty\n",
        "if penalty == 'l1':\n",
        "    solver = 'saga'\n",
        "elif penalty == 'elasticnet':\n",
        "    solver = 'saga'\n",
        "else:\n",
        "    solver = 'lbfgs'\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Getting the best parameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Output the best parameters\n",
        "print(f\"The best parameters are: {best_params}\")\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output the accuracy\n",
        "print(f\"The accuracy of the best model on the test data is: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "biysX07YOXaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best accuracy I got is 0.78, let's automate this"
      ],
      "metadata": {
        "id": "ndpBM4itOuqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRID SEARCH"
      ],
      "metadata": {
        "id": "BAvIJhByGVnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.01, 0.1, 1, 10],  # Example values\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "if 'elasticnet' in param_grid['penalty']:\n",
        "    param_grid['l1_ratio'] = [0.1, 0.5, 0.9]  # Example values\n"
      ],
      "metadata": {
        "id": "O_B-xvGjJYYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = GridSearchCV(LogisticRegression(max_iter=100), param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters found: {clf.best_params_}\")\n",
        "print(f\"Best cross-validation score: {clf.best_score_:.2f}\")\n"
      ],
      "metadata": {
        "id": "PWPFdTWWJaX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output analysis : we are not getting much better result, even worse accuracy at 0.76. Let's try another way. Grid search is limiting us to specific values on the grid."
      ],
      "metadata": {
        "id": "AAbbU8PpNIm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RANDOM SEARCH"
      ],
      "metadata": {
        "id": "RHIfmZmKKNbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of hyperparameters you want to search over\n",
        "param_dist = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'l1_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='saga')\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Getting the best parameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Output the best parameters\n",
        "print(f\"The best parameters are: {best_params}\")\n",
        "\n",
        "# Calculate the accuracy on the test set using the best model\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output the accuracy\n",
        "print(f\"The accuracy of the best model on the test data is: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "sbcswDcJKWqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy is slightly improve with random search to 0,78"
      ],
      "metadata": {
        "id": "6DaIHYI7KrYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BAYESIAN OPTIMIZATION"
      ],
      "metadata": {
        "id": "IFVWFlWgPAVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a sample dataset (you can replace this with your dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter search space\n",
        "param_space = {\n",
        "    'C': (1e-6, 1e+6, 'log-uniform'),  # Search for C values in log scale\n",
        "    'gamma': (1e-6, 1e+1, 'log-uniform'),  # Search for gamma values in log scale\n",
        "    'kernel': ['linear', 'rbf'],  # Choose between linear and RBF kernels\n",
        "}\n",
        "\n",
        "# Initialize the Bayesian Optimization search\n",
        "opt = BayesSearchCV(\n",
        "    SVC(),\n",
        "    param_space,\n",
        "    n_iter=50,  # Number of optimization steps\n",
        "    cv=5,  # Cross-validation folds\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    verbose=1,  # Print progress\n",
        "    random_state=42,  # Set a random seed for reproducibility\n",
        ")\n",
        "\n",
        "# Perform the Bayesian optimization search\n",
        "opt.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = opt.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters on the test set\n",
        "best_model = opt.best_estimator_\n",
        "test_score = best_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Test Accuracy with Best Hyperparameters:\", test_score)\n"
      ],
      "metadata": {
        "id": "ZwgeC7MaPIwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we achieved 1 which is a huge warning sign for overfitting, something that can happen quite often with Bayesian optimization. It's then crucial to assess the model's generalization performance on unseen data to confirm its effectiveness.\n",
        "\n",
        "It would be also essential to consider other evaluation metrics, such as precision, recall, and F1-score, in addition to accuracy, to get a more comprehensive understanding of model performance."
      ],
      "metadata": {
        "id": "bbhxuBvRQ-5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Comparing different models\n",
        "\n",
        "Optimization is not getting us much further, either remaining low in accuracy or risking overfitting. Let's evaluate different models for the same dataset and pick the most accurate one using PyCaret. Let's start from the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "_1CVjQ5ypS1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical-Data-Analytics-with-Python/afab193c5cb3d6878755c4d12e8baa821a8ab054/Datasets/23/framingham.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "w1a9dSzRpUNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Pycart ?\n",
        "PyCaret is an open-source Python library designed to simplify and accelerate the end-to-end machine learning and model deployment process. It provides a high-level, easy-to-use interface that automates many of the repetitive tasks and reduces the amount of code required to develop and deploy machine learning models.\n",
        "\n",
        "### Key features of PyCaret include:\n",
        "Automated Setup: PyCaret simplifies data preprocessing by automatically handling common tasks like data cleaning, feature selection, and feature engineering. It can also automatically split the data into training and testing sets.\n",
        "\n",
        "Model Selection: PyCaret allows you to easily compare and evaluate multiple machine learning models, including classification, regression, clustering, and anomaly detection algorithms. It provides a single function to create, train, and evaluate models, making it straightforward to identify the best-performing model for your task.\n",
        "\n",
        "Hyperparameter Tuning: PyCaret supports hyperparameter tuning for models, helping you find the best combination of hyperparameters to optimize model performance.\n",
        "\n",
        "Model Interpretability: It provides tools for interpreting and visualizing model results, including feature importance plots, confusion matrices, and ROC curves.\n",
        "\n",
        "Deployment: PyCaret includes functionality for deploying machine learning models to production, making it easier to transition from model development to real-world applications.\n",
        "\n",
        "Anomaly Detection: PyCaret also supports anomaly detection, which is useful for identifying rare or unusual data points.\n",
        "\n",
        "Natural Language Processing (NLP): It has modules for NLP tasks, such as text classification and sentiment analysis.\n",
        "\n",
        "Time Series Analysis: PyCaret supports time series forecasting and analysis."
      ],
      "metadata": {
        "id": "9VV-5mPVR64j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compare models using pycaret"
      ],
      "metadata": {
        "id": "YTgIflgCRiaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret setup\n",
        "setup(data=df, target='TenYearCHD', verbose=False, session_id=123)\n",
        "\n",
        "# Compare models\n",
        "compare_models()"
      ],
      "metadata": {
        "id": "g-RUsi6jRm9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Output analysis :  the Logistic Regression model has the highest accuracy of 0.8510, highest AUC of 0.7201, and reasonable recall and precision. The training time is also very fast at 1.749 seconds.\n",
        "\n",
        "The other top performers are the LDA, Ridge, and Dummy classifiers, which have slightly lower accuracy than LogisticRegression.\n",
        "\n",
        "The ensemble methods like Random Forest, AdaBoost, Extra Trees, LightGBM, and XGBoost have good accuracy around 0.84-0.85, but lower AUC and recall/precision than LogisticRegression. Their training times are also slower.\n",
        "\n",
        "The KNN, SVM, QDA, and Naive Bayes models have decent accuracy between 0.82-0.83, but weaker AUC and recall/precision.\n",
        "\n",
        "The Decision Tree classifier has much lower performance with 0.7603 accuracy.\n",
        "\n",
        "Overall, the LogisticRegression model appears to be the best for predicting positive cases based on its top accuracy and AUC, good balance of recall and precision, and fast training time. The ensemble methods are good alternatives, while the other models are outperformed by LogisticRegression."
      ],
      "metadata": {
        "id": "Ztm0odIST1Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's roll it again but this time after dealing with missing values with median"
      ],
      "metadata": {
        "id": "o4tW5rAdSh01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values based on user's choice\n",
        "def handle_missing_values(df, option):\n",
        "    \"\"\"Handle missing values based on user's choice.\"\"\"\n",
        "    if option == 'Remove Rows':\n",
        "        df.dropna(inplace=True)\n",
        "    elif option == 'Replace with Mean':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].mean(), inplace=True)\n",
        "    elif option == 'Replace with Median':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].median(), inplace=True)\n",
        "    elif option == 'Replace with Mode':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "    elif option == 'Forward or Backward Fill':\n",
        "        df.fillna(method='ffill', inplace=True)\n",
        "        df.fillna(method='bfill', inplace=True)\n",
        "    return df\n",
        "\n",
        "# User's choice using Google Colab form field dropdown\n",
        "missing_value_option = 'Remove Rows' #@param [\"Remove Rows\", \"Replace with Mean\", \"Replace with Median\", \"Replace with Mode\", \"Forward or Backward Fill\"]\n",
        "\n",
        "# Handle missing values based on user's choice\n",
        "df_nm = handle_missing_values(df, missing_value_option)\n"
      ],
      "metadata": {
        "id": "WfIbRlzMSx4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret setup\n",
        "setup(data=df_nm, target='TenYearCHD', verbose=False, session_id=123)\n",
        "\n",
        "# Compare models\n",
        "compare_models()"
      ],
      "metadata": {
        "id": "hYgJIAYDS1V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much difference with or without missing values"
      ],
      "metadata": {
        "id": "IADj0eBBUrUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now after dealing with imbalance in the predictors"
      ],
      "metadata": {
        "id": "J-xVnAt8Tp1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rdy = df_nm.copy()\n",
        "\n",
        "# Function to handle imbalance\n",
        "def handle_imbalance(df, column, method):\n",
        "    if method == \"Oversampling\":\n",
        "        df_majority = df[df[column]==0]\n",
        "        df_minority = df[df[column]==1]\n",
        "        df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=123)\n",
        "        df = pd.concat([df_majority, df_minority_upsampled])\n",
        "    elif method == \"Undersampling\":\n",
        "        df_majority = df[df[column]==0]\n",
        "        df_minority = df[df[column]==1]\n",
        "        df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=123)\n",
        "        df = pd.concat([df_minority, df_majority_downsampled])\n",
        "    elif method == \"SMOTE\":\n",
        "        X = df.drop(columns=[column])\n",
        "        y = df[column]\n",
        "        smote = SMOTE(random_state=123)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "        df = pd.concat([X, y], axis=1)\n",
        "    return df\n",
        "\n",
        "# Choice of method to handle imbalance for BPMeds\n",
        "method_BPMeds = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'BPMeds', method_BPMeds)\n",
        "\n",
        "# Choice of method to handle imbalance for prevalentStroke\n",
        "method_prevalentStroke = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'prevalentStroke', method_prevalentStroke)\n",
        "\n",
        "# Choice of method to handle imbalance for diabetes\n",
        "method_diabetes = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'diabetes', method_diabetes)\n",
        "\n",
        "df_rdy.head()"
      ],
      "metadata": {
        "id": "-j8NHplqTtJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret setup\n",
        "setup(data=df_rdy, target='TenYearCHD', verbose=False, session_id=123)\n",
        "\n",
        "# Compare models\n",
        "compare_models()"
      ],
      "metadata": {
        "id": "PAksajMPTxML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are reaching high levels of accuracy but we did not consider the risk for imbalance and did not workout the normalization. Let's get that on."
      ],
      "metadata": {
        "id": "GecWPEUoUw2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret setup\n",
        "setup(data=df_rdy, target='TenYearCHD', verbose=False, session_id=123, normalize=True, transformation=True, transformation_method='yeo-johnson')\n",
        "\n",
        "# Handle class imbalance using PyCaret's `create_model` function\n",
        "# You can specify the method as 'SMOTE' or other methods\n",
        "model = create_model('lr')  # Logistic Regression model\n",
        "\n",
        "# Compare models\n",
        "compare_models()\n"
      ],
      "metadata": {
        "id": "JW24Bow8VY7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is still very high which suggest overfitting again. Let's check optimizing."
      ],
      "metadata": {
        "id": "NAkn4xk8W3b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PyCaret setup\n",
        "setup(data=df_rdy, target='TenYearCHD', verbose=False, session_id=123, normalize=True, transformation=True, transformation_method='yeo-johnson')\n",
        "\n",
        "# Create an Extra Trees Classifier\n",
        "et_classifier = create_model('et')  # You can adjust other parameters here\n",
        "\n",
        "# Tune hyperparameters of the Extra Trees Classifier\n",
        "tuned_et_classifier = tune_model(et_classifier)\n",
        "\n",
        "# You can also specify a custom hyperparameter grid for tuning:\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_depth': [None, 10, 20, 30],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "# }\n",
        "# tuned_et_classifier = tune_model(et_classifier, custom_grid=param_grid)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "evaluate_model(tuned_et_classifier)\n",
        "\n",
        "# You can use this tuned model for predictions\n",
        "# For example:\n",
        "# predictions = predict_model(tuned_et_classifier, data=df)\n"
      ],
      "metadata": {
        "id": "BXT9Ph11XHtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I highly suspect overfitting so let's :\n",
        "\n",
        "Increase fold to perform more cross validation\n"
      ],
      "metadata": {
        "id": "VvOGILaVXoKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setup(data=df_rdy, target='TenYearCHD', verbose=False, session_id=123, fold=10)  # Increase the number of folds (e.g., fold=10)\n",
        "\n",
        "# Create an Extra Trees Classifier\n",
        "et_classifier = create_model('et')  # You can adjust other parameters here\n",
        "\n",
        "# Tune hyperparameters of the Extra Trees Classifier\n",
        "tuned_et_classifier = tune_model(et_classifier)\n",
        "\n",
        "# You can also specify a custom hyperparameter grid for tuning:\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_depth': [None, 10, 20, 30],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "# }\n",
        "# tuned_et_classifier = tune_model(et_classifier, custom_grid=param_grid)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "evaluate_model(tuned_et_classifier)\n",
        "\n",
        "# You can use this tuned model for predictions\n",
        "# For example:\n",
        "# predictions = predict_model(tuned_et_classifier, data=df)\n"
      ],
      "metadata": {
        "id": "X4WqLTt2YXbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test ensemble methods like Bagging or Boosting. These methods can help reduce overfitting by combining multiple models. PyCaret provides functions like ensemble_model to create ensemble models."
      ],
      "metadata": {
        "id": "AXWP7HCQX-wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model(et_classifier)"
      ],
      "metadata": {
        "id": "0O1oClMZYWjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final approach might be to drop some features that might be causing overfitting."
      ],
      "metadata": {
        "id": "5CvqvXObYPCg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvKbcAfyYW7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Optimization\n",
        "\n",
        "In this step, we aim to fine-tune the best model's parameters to enhance its performance using techniques like Grid Search or Random Search.\n"
      ],
      "metadata": {
        "id": "cR_TuwXwpUnW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huz2zl7BE7x-"
      },
      "source": [
        "# Conclusions and perspectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PdTFmI5FJb8"
      },
      "source": [
        "In this notebook, we"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}