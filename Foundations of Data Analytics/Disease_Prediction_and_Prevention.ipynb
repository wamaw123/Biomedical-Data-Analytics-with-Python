{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wamaw123/Biomedical-Data-Analytics-with-Python/blob/main/Foundations%20of%20Data%20Analytics/Disease_Prediction_and_Prevention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i8SP3ReLr0I"
      },
      "source": [
        "# Data Analytics with Python\n",
        "By : [Abderrahim Benmoussa, Ph.D. ](https://https://github.com/wamaw123)\n",
        "\n",
        "Project's on Github : https://github.com/wamaw123/Biomedical_Data_analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33r-b9O2cBwK"
      },
      "source": [
        "# Disease Prediction and Prevention\n",
        "\n",
        "In this notebook, we aim to predict the 10-year risk of future coronary heart disease (CHD) using the Framingham Heart Study dataset. This will be a binary classification task, where `1` indicates the risk of CHD, and `0` indicates no risk. To do so we will explore the dataset and go on to test a simple logistic regression model first. We will then compare different models and finally optimize the model to get the best predictive accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install and Import Libraries\n",
        "\n",
        "In this step, we will install and import necessary libraries for our analysis.\n",
        "- `pandas` and `numpy` for data manipulation\n",
        "- `seaborn` and `matplotlib` for data visualization\n",
        "- `scikit-learn` for building and evaluating the machine learning model"
      ],
      "metadata": {
        "id": "iv0Svz3JoO4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIbD25DnbYKc"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas numpy scipy statsmodels patsy dtale scikit-learn pandas_profiling\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "## Data Manipulation\n",
        "import pandas as pd   # Essential for data manipulation and mathematical operations.\n",
        "import numpy as np    # Used for array-based operations and mathematical functions.\n",
        "\n",
        "## Visualization\n",
        "import matplotlib.pyplot as plt  # Fundamental plotting library.\n",
        "import seaborn as sns            # Builds on top of matplotlib for more advanced visualizations.\n",
        "\n",
        "## Statistical Testing, modeling and data preparation\n",
        "from scipy import stats           # Library for scientific and technical computing.\n",
        "import statsmodels.api as sm      # Provides classes and functions for the estimation of many different statistical models.\n",
        "import statsmodels.formula.api as smf  # Formula-based API for the statsmodels library.\n",
        "from sklearn.model_selection import train_test_split  # Import train_test_split function to split data into training and testing sets.\n",
        "from sklearn.preprocessing import StandardScaler  # Import StandardScaler to standardize features by removing the mean and scaling to unit variance.\n",
        "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression to perform logistic regression.\n",
        "from sklearn.metrics import classification_report, accuracy_score  # Import classification_report to build a text report showing the main classification metrics, and accuracy_score to compute the accuracy of the algorithm.\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "## Interactive Exploration\n",
        "from collections import Counter\n",
        "import pandas_profiling as pp\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXnchmZHfu8E"
      },
      "source": [
        "## Step 2: Load the Dataset\n",
        "\n",
        "Next we load the week 2 dataset directly from GitHub and set it into a Pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdQjhLwPf5ZH"
      },
      "outputs": [],
      "source": [
        "# Fetch the dataset from GitHub\n",
        "url = \"https://raw.githubusercontent.com/wamaw123/Biomedical-Data-Analytics-with-Python/afab193c5cb3d6878755c4d12e8baa821a8ab054/Datasets/23/framingham.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UraLwWhrfXQ6"
      },
      "source": [
        "#About the dataset\n",
        "\n",
        "## Source\n",
        "The dataset is publicly available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD). The dataset provides the patientsâ€™ information. It includes over 4,000 records and 15 attributes.\n",
        "\n",
        "## Variables\n",
        "Each attribute is a potential risk factor. There are both demographic, behavioral, and medical risk factors.\n",
        "\n",
        "### Demographic:\n",
        "- **Sex:** male or female (Nominal)\n",
        "- **Age:** Age of the patient; (Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
        "\n",
        "### Behavioral:\n",
        "- **Current Smoker:** whether or not the patient is a current smoker (Nominal)\n",
        "- **Cigs Per Day:** the number of cigarettes that the person smoked on average in one day. (Can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
        "\n",
        "### Medical (history):\n",
        "- **BP Meds:** whether or not the patient was on blood pressure medication (Nominal)\n",
        "- **Prevalent Stroke:** whether or not the patient had previously had a stroke (Nominal)\n",
        "- **Prevalent Hyp:** whether or not the patient was hypertensive (Nominal)\n",
        "- **Diabetes:** whether or not the patient had diabetes (Nominal)\n",
        "\n",
        "### Medical (current):\n",
        "- **Tot Chol:** total cholesterol level (Continuous)\n",
        "- **Sys BP:** systolic blood pressure (Continuous)\n",
        "- **Dia BP:** diastolic blood pressure (Continuous)\n",
        "- **BMI:** Body Mass Index (Continuous)\n",
        "- **Heart Rate:** heart rate (Continuous - In medical research, variables such as heart rate, though in fact discrete, yet are considered continuous because of a large number of possible values.)\n",
        "- **Glucose:** glucose level (Continuous)\n",
        "\n",
        "### Predict variable (desired target):\n",
        "- **10-year risk of coronary heart disease CHD (binary:** \"1\" means \"Yes,\" \"0\" means \"No\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Preprocessing\n",
        "\n",
        "We will perform initial data preprocessing such as handling missing values. This step is crucial to ensure the quality and reliability of our machine learning model. There are many ways to deal with missing values for instance. Those can be droped or inputed in different ways. Let's first check what missing values we have."
      ],
      "metadata": {
        "id": "tqApg3NKos5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "# isnull() returns a DataFrame where each cell is either True or False depending on that cell's null status.\n",
        "# sum() will then sum the True values (count of missing values) for each column.\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "# Display the percentage of missing values for each column\n",
        "print(f\"Percentage of missing values for each column:\\n{missing_percentage}\")\n",
        "\n",
        "# Visualize the missing values as a heatmap\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "sns.heatmap(df.isnull(),     # Provide DataFrame with null-status information\n",
        "            cbar=False,      # Do not draw a color bar\n",
        "            cmap='viridis')  # Use the viridis color map\n",
        "\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gY69h8Dvozmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : Education and the values for glycemia seem to be the ones with most missing values. There are no missing values for the 10 year CHD. There are many ways to deal with missing values.\n",
        "\n",
        "- Remove Rows with Missing Values\n",
        "- Replace Missing Values with Mean\n",
        "- Replace Missing Values with Median\n",
        "- Replace Missing Values with Mode\n",
        "- Use Forward or Backward Fill\n",
        "\n",
        "I would prefer either dropping the values since only 10% are missing for glycemia or using median. I don't expect much difference between the two, so I will go with either of them."
      ],
      "metadata": {
        "id": "hXYd69j5s2gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample code to illustrate handling missing values based on user's choice\n",
        "def handle_missing_values(df, option):\n",
        "    \"\"\"Handle missing values based on user's choice.\"\"\"\n",
        "    if option == 'Remove Rows':\n",
        "        df.dropna(inplace=True)\n",
        "    elif option == 'Replace with Mean':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].mean(), inplace=True)\n",
        "    elif option == 'Replace with Median':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].median(), inplace=True)\n",
        "    elif option == 'Replace with Mode':\n",
        "        for column in df.columns:\n",
        "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "    elif option == 'Forward or Backward Fill':\n",
        "        df.fillna(method='ffill', inplace=True)\n",
        "        df.fillna(method='bfill', inplace=True)\n",
        "    return df\n",
        "\n",
        "# User's choice using Google Colab form field dropdown\n",
        "missing_value_option = 'Replace with Median' #@param [\"Remove Rows\", \"Replace with Mean\", \"Replace with Median\", \"Replace with Mode\", \"Forward or Backward Fill\"]\n",
        "\n",
        "# Handle missing values based on user's choice\n",
        "df_nm = handle_missing_values(df, missing_value_option)\n"
      ],
      "metadata": {
        "id": "esguogxLtzbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check again for missing values"
      ],
      "metadata": {
        "id": "pUfbgBsVu1Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "# isnull() returns a DataFrame where each cell is either True or False depending on that cell's null status.\n",
        "# sum() will then sum the True values (count of missing values) for each column.\n",
        "print(df_nm.isnull().sum())\n",
        "\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = (df_nm.isnull().sum() / len(df_nm)) * 100\n",
        "\n",
        "# Display the percentage of missing values for each column\n",
        "print(f\"Percentage of missing values for each column:\\n{missing_percentage}\")\n",
        "\n",
        "# Visualize the missing values as a heatmap\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "sns.heatmap(df_nm.isnull(),     # Provide DataFrame with null-status information\n",
        "            cbar=False,      # Do not draw a color bar\n",
        "            cmap='viridis')  # Use the viridis color map\n",
        "\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HHwdQohFu3Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the output : The data is now devoid of missing values."
      ],
      "metadata": {
        "id": "6EoDo49Gvm4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now check for correlations between these variables"
      ],
      "metadata": {
        "id": "ZCJHOdPpsl7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(12, 8))  # Set the size of the figure\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,  # Annotate each cell with the numeric value\n",
        "            cmap='coolwarm',  # Use a cool-warm color map\n",
        "            vmin=-1, vmax=1,  # Set color scale limits\n",
        "            linewidths=.5)  # Set linewidth between entries in matrix\n",
        "\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6W4KF0pwspkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjiuuE63hxuh"
      },
      "source": [
        "Analysis of the output : we see some interesting correlations with our dependant variable (target). Mostly age and hypertension hallmarks, which is fairly expectable from a scientific point of view."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Exploratory Data Analysis (EDA) and\n",
        "\n",
        "Let's explore the dataset to understand it better and figure out how to approach the prediction problem. Visualization helps in identifying patterns and anomalies in the dataset. It will be crucial to deal with imbalanced dataset issues by using techniques like oversampling, undersampling, or SMOTE.\n"
      ],
      "metadata": {
        "id": "ikxKynFio4qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting information on the data form\n",
        "df_nm.info()"
      ],
      "metadata": {
        "id": "ieRMvXyEo5ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output interpretation: The table contains 4,238 entries (or rows) and 16 different categories (or columns) of information. These categories include things like gender (male), age, education, whether the person is a current smoker, and the number of cigarettes smoked per day, among others. All entries in the table are non-null, meaning they all contain data, and the data is in different formats, including integers (int64) and floating-point numbers (float64). The table takes up about 530 kilobytes of memory space."
      ],
      "metadata": {
        "id": "Qm2xvU4wxN_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's further explore the data using panda profiling"
      ],
      "metadata": {
        "id": "sIRq6RZryuBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp.ProfileReport(df_nm)"
      ],
      "metadata": {
        "id": "wdse1msvy5e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall informations gathered from Panda Profiling :\n",
        "\n",
        "- There's a strong relationship between the number of cigarettes smoked per day (cigsPerDay) and whether the person is a current smoker (currentSmoker).\n",
        "- Systolic blood pressure (sysBP) is closely related to diastolic blood pressure (diaBP) and one other field.\n",
        "- Diastolic blood pressure (diaBP) is also closely related to systolic blood pressure (sysBP) and one other field.\n",
        "- There's a strong link between glucose levels and diabetes.\n",
        "- PrevalentHyp is strongly connected with sysBP and one other field.\n",
        "- The BPMeds, prevalentStroke, and diabetes fields are highly imbalanced, meaning most of the values are the same (80.8% for BPMeds, 94.8% for prevalentStroke, and 82.8% for diabetes).\n",
        "- Half of the cigsPerDay entries are zero, indicating a lot of non-smokers or missing data : in this case, probably non-smokers since missing data were initial NaNs."
      ],
      "metadata": {
        "id": "UXlnX_U2zVDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imbalance in Data and Why It's an Issue\n",
        "\n",
        "\"Imbalance\" in data refers to a situation where the distribution of data among different categories or classes is unequal. One class may have significantly more instances than others.\n",
        "\n",
        "## Issues Caused by Imbalance:\n",
        "\n",
        "1. **Model Bias:**\n",
        "   - An imbalanced dataset can cause a predictive model to be biased towards the majority class, resulting in poor performance for the minority class.\n",
        "\n",
        "2. **Misleading Accuracy:**\n",
        "   - A model might show high accuracy by simply predicting the majority class, providing a false sense of effectiveness.\n",
        "\n",
        "3. **Loss of Information:**\n",
        "   - Patterns associated with the minority class may be overlooked, leading to a lack of important insights.\n",
        "\n",
        "4. **Overfitting:**\n",
        "   - The model may memorize the few available minority class instances rather than generalizing, leading to overfitting.\n",
        "\n",
        "5. **False Assumptions:**\n",
        "   - Incorrect assumptions may be made about real-world class distributions, affecting model performance in practical applications.\n",
        "\n",
        "## Solutions:\n",
        "\n",
        "1. **Resampling:**\n",
        "   - Oversample the minority class or undersample the majority class.\n",
        "\n",
        "2. **Synthetic Data Generation:**\n",
        "   - Generate new data points for the minority class, e.g., using the Synthetic Minority Over-sampling Technique (SMOTE).\n",
        "\n",
        "3. **Cost-sensitive Learning:**\n",
        "   - Penalize the misclassification of the minority class more heavily.\n",
        "\n",
        "4. **Using Different Evaluation Metrics:**\n",
        "   - Utilize metrics like the F1-score, precision, recall, or Area Under the Receiver Operating Characteristic (ROC) curve to evaluate model performance.\n",
        "\n",
        "For the sake of this excercise, let's explore resampling and more specifically SMOTE.\n",
        "\n"
      ],
      "metadata": {
        "id": "_LcmhNPz0380"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Necessary Libraries\n",
        "\n",
        "# Assumed data loading\n",
        "# df = pd.read_csv('link_to_dataset')\n",
        "\n",
        "df_rdy = df_nm.copy()\n",
        "\n",
        "# Function to handle imbalance\n",
        "def handle_imbalance(df, column, method):\n",
        "    if method == \"Oversampling\":\n",
        "        df_majority = df[df[column]==0]\n",
        "        df_minority = df[df[column]==1]\n",
        "        df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=123)\n",
        "        df = pd.concat([df_majority, df_minority_upsampled])\n",
        "    elif method == \"Undersampling\":\n",
        "        df_majority = df[df[column]==0]\n",
        "        df_minority = df[df[column]==1]\n",
        "        df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=123)\n",
        "        df = pd.concat([df_minority, df_majority_downsampled])\n",
        "    elif method == \"SMOTE\":\n",
        "        X = df.drop(columns=[column])\n",
        "        y = df[column]\n",
        "        smote = SMOTE(random_state=123)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "        df = pd.concat([X, y], axis=1)\n",
        "    return df\n",
        "\n",
        "# Choice of method to handle imbalance for BPMeds\n",
        "method_BPMeds = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'BPMeds', method_BPMeds)\n",
        "\n",
        "# Choice of method to handle imbalance for prevalentStroke\n",
        "method_prevalentStroke = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'prevalentStroke', method_prevalentStroke)\n",
        "\n",
        "# Choice of method to handle imbalance for diabetes\n",
        "method_diabetes = 'SMOTE' # @param [\"Oversampling\", \"Undersampling\", \"SMOTE\"]\n",
        "df_rdy = handle_imbalance(df_rdy, 'diabetes', method_diabetes)\n",
        "\n",
        "# Handling relationships and zero values\n",
        "# Creating an interaction term for sysBP and diaBP\n",
        "df_rdy['bp_interaction'] = df_rdy['sysBP'] * df_rdy['diaBP']\n",
        "\n",
        "# Handling cigsPerDay (since smokers are not missing values, no change is made here)\n",
        "\n",
        "df_rdy.head()"
      ],
      "metadata": {
        "id": "m44EZNNj3N2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore again the dataset now that those issues have been adressed"
      ],
      "metadata": {
        "id": "By3AsBVL332z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pp.ProfileReport(df_rdy)"
      ],
      "metadata": {
        "id": "Ic379mkp39Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Step 5: Feature Selection and Splitting the Dataset\n",
        "\n",
        "In this step, we will select the relevant features for our model and split the dataset into training and testing sets. This separation allows us to evaluate the model's performance on unseen data.  "
      ],
      "metadata": {
        "id": "HG-sSDp2pG0A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bhw1qTWMpKZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Feature Scaling\n",
        "\n",
        "Standardizing the dataset is crucial for many machine learning models and helps the model to converge faster.\n"
      ],
      "metadata": {
        "id": "AkJL4Hh8pK9H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEKUQhFGpM53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 7: Model Building\n",
        "\n",
        "We will use the Logistic Regression model for prediction, which is a commonly used algorithm for binary classification problems.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AJgB1vNqpNXi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9Co564OpQYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Model Evaluation\n",
        "\n",
        "Evaluate the model performance by comparing the predicted and actual values. We will use metrics such as precision, recall, and accuracy to assess the model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "-4l8IdakpQu-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PbsIGpzpShw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Comparing different models\n",
        "\n",
        "We will evaluate different models for the same dataset and pick the most accurate one using PyCaret.\n",
        "\n"
      ],
      "metadata": {
        "id": "_1CVjQ5ypS1z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1a9dSzRpUNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Optimization\n",
        "\n",
        "In this step, we aim to fine-tune the best model's parameters to enhance its performance using techniques like Grid Search or Random Search.\n"
      ],
      "metadata": {
        "id": "cR_TuwXwpUnW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huz2zl7BE7x-"
      },
      "source": [
        "# Conclusions and perspectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PdTFmI5FJb8"
      },
      "source": [
        "In this notebook, we"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}